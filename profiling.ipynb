{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "040a2c52-9202-4fbf-8f0e-12268788561c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prof_mil_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prof_mil_script.py\n",
    "# Copyright (c) MONAI Consortium\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import argparse\n",
    "import collections.abc\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "import gdown\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "from monai.config import KeysCollection\n",
    "from monai.data import Dataset, load_decathlon_datalist\n",
    "from monai.data.wsi_reader import WSIReader\n",
    "from monai.metrics import Cumulative, CumulativeAverage\n",
    "from monai.networks.nets import milmodel\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    GridPatchd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    RandFlipd,\n",
    "    RandGridPatchd,\n",
    "    RandRotate90d,\n",
    "    ScaleIntensityRanged,\n",
    "    SplitDimd,\n",
    "    ToTensord,\n",
    ")\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import nvtx\n",
    "\n",
    "@nvtx.annotate(\"train_epoch\", color=\"blue\")\n",
    "def train_epoch(model, loader, optimizer, scaler, epoch, epochs):\n",
    "    \"\"\"One train epoch over the dataset\"\"\"\n",
    "\n",
    "    model.train()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    run_loss = CumulativeAverage()\n",
    "    run_acc = CumulativeAverage()\n",
    "\n",
    "    start_time = time.time()\n",
    "    loss, acc = 0.0, 0.0\n",
    "\n",
    "    for idx, batch_data in enumerate(loader):\n",
    "        with nvtx.annotate(\"Start batch\", color=\"green\"):\n",
    "            with nvtx.annotate(\"get data\", color=\"yellow\"):\n",
    "                data = batch_data[\"image\"].as_subclass(torch.Tensor).cuda(0)\n",
    "                target = batch_data[\"label\"].as_subclass(torch.Tensor).cuda(0)\n",
    "                \n",
    "            with nvtx.annotate(\"zero optimizer\", color=\"cyan\"):\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with nvtx.annotate(\"do inference\", color=\"red\"):    \n",
    "                with autocast(enabled=True):\n",
    "                    logits = model(data)\n",
    "                    loss = criterion(logits, target)\n",
    "\n",
    "            with nvtx.annotate(\"back_prop\", color=\"grey\"): \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "            with nvtx.annotate(\"metrics\", color=\"purple\"):\n",
    "                acc = (logits.sigmoid().sum(1).detach().round() == target.sum(1).round()).float().mean()\n",
    "\n",
    "                run_loss.append(loss)\n",
    "                run_acc.append(acc)\n",
    "\n",
    "                loss = run_loss.aggregate()\n",
    "                acc = run_acc.aggregate()\n",
    "\n",
    "                print(\n",
    "                    \"Epoch {}/{} {}/{}\".format(epoch, epochs, idx, len(loader)),\n",
    "                    \"loss: {:.4f}\".format(loss),\n",
    "                    \"acc: {:.4f}\".format(acc),\n",
    "                    \"time {:.2f}s\".format(time.time() - start_time),\n",
    "                )\n",
    "                start_time = time.time()\n",
    "\n",
    "    return loss, acc\n",
    "\n",
    "@nvtx.annotate(\"val_epoch\", color=\"blue\")\n",
    "def val_epoch(model, loader, epoch, epochs, max_tiles=None):\n",
    "    \"\"\"One validation epoch over the dataset\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    model2 = model\n",
    "    calc_head = model2.calc_head\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    run_loss = CumulativeAverage()\n",
    "    run_acc = CumulativeAverage()\n",
    "    PREDS = Cumulative()\n",
    "    TARGETS = Cumulative()\n",
    "\n",
    "    start_time = time.time()\n",
    "    loss, acc = 0.0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch_data in enumerate(loader):\n",
    "            data = batch_data[\"image\"].as_subclass(torch.Tensor).cuda(0)\n",
    "            target = batch_data[\"label\"].as_subclass(torch.Tensor).cuda(0)\n",
    "\n",
    "            with autocast(enabled=True):\n",
    "                if max_tiles is not None and data.shape[1] > max_tiles:\n",
    "                    # During validation, we want to use all instances/patches\n",
    "                    # and if its number is very big, we may run out of GPU memory\n",
    "                    # in this case, we first iteratively go over subsets of patches to calculate backbone features\n",
    "                    # and at the very end calculate the classification output\n",
    "\n",
    "                    logits = []\n",
    "                    logits2 = []\n",
    "\n",
    "                    for i in range(int(np.ceil(data.shape[1] / float(max_tiles)))):\n",
    "                        data_slice = data[:, i * max_tiles : (i + 1) * max_tiles]\n",
    "                        logits_slice = model(data_slice, no_head=True)\n",
    "                        logits.append(logits_slice)\n",
    "                        \n",
    "                    logits = torch.cat(logits, dim=1)\n",
    "                    logits = calc_head(logits)\n",
    "\n",
    "                else:\n",
    "                    # if number of instances is not big, we can run inference directly\n",
    "                    logits = model(data)\n",
    "\n",
    "                loss = criterion(logits, target)\n",
    "\n",
    "            pred = logits.sigmoid().sum(1).detach().round()\n",
    "            target = target.sum(1).round()\n",
    "            acc = (pred == target).float().mean()\n",
    "\n",
    "            run_loss.append(loss)\n",
    "            run_acc.append(acc)\n",
    "            loss = run_loss.aggregate()\n",
    "            acc = run_acc.aggregate()\n",
    "\n",
    "            PREDS.extend(pred)\n",
    "            TARGETS.extend(target)\n",
    "\n",
    "            print(\n",
    "                \"Val epoch {}/{} {}/{}\".format(epoch, epochs, idx, len(loader)),\n",
    "                \"loss: {:.4f}\".format(loss),\n",
    "                \"acc: {:.4f}\".format(acc),\n",
    "                \"time {:.2f}s\".format(time.time() - start_time),\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "        # Calculate QWK metric (Quadratic Weigted Kappa) https://en.wikipedia.org/wiki/Cohen%27s_kappa\n",
    "        PREDS = PREDS.get_buffer().cpu().numpy()\n",
    "        TARGETS = TARGETS.get_buffer().cpu().numpy()\n",
    "        qwk = cohen_kappa_score(PREDS.astype(np.float64), TARGETS.astype(np.float64), weights=\"quadratic\")\n",
    "\n",
    "    return loss, acc, qwk\n",
    "\n",
    "@nvtx.annotate(\"LabelEncodeIntegerGraded\", color=\"blue\")\n",
    "class LabelEncodeIntegerGraded(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert an integer label to encoded array representation of length num_classes,\n",
    "    with 1 filled in up to label index, and 0 otherwise. For example for num_classes=5,\n",
    "    embedding of 2 -> (1,1,0,0,0)\n",
    "\n",
    "    Args:\n",
    "        num_classes: the number of classes to convert to encoded format.\n",
    "        keys: keys of the corresponding items to be transformed. Defaults to ``'label'``.\n",
    "        allow_missing_keys: don't raise exception if key is missing.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        keys: KeysCollection = \"label\",\n",
    "        allow_missing_keys: bool = False,\n",
    "    ):\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            label = int(d[key])\n",
    "\n",
    "            lz = np.zeros(self.num_classes, dtype=np.float32)\n",
    "            lz[:label] = 1.0\n",
    "            # alternative oneliner lz=(np.arange(self.num_classes)<int(label)).astype(np.float32) #same oneliner\n",
    "            d[key] = lz\n",
    "\n",
    "        return d\n",
    "\n",
    "@nvtx.annotate(\"list_data_collate\", color=\"blue\")\n",
    "def list_data_collate(batch: collections.abc.Sequence):\n",
    "    \"\"\"\n",
    "    Combine instances from a list of dicts into a single dict, by stacking them along first dim\n",
    "    [{'image' : 3xHxW}, {'image' : 3xHxW}, {'image' : 3xHxW}...] - > {'image' : Nx3xHxW}\n",
    "    followed by the default collate which will form a batch BxNx3xHxW\n",
    "    \"\"\"\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        # print(f\"{i} = {item['image'].shape=} >> {item['image'].keys=}\")\n",
    "        data = item[0]\n",
    "        data[\"image\"] = torch.stack([ix[\"image\"] for ix in item], dim=0)\n",
    "        # data[\"patch_location\"] = torch.stack([ix[\"patch_location\"] for ix in item], dim=0)\n",
    "        batch[i] = data\n",
    "    return default_collate(batch)\n",
    "\n",
    "\n",
    "def main_worker(args):\n",
    "\n",
    "    torch.cuda.set_device(0)  # use this default device (same as args.device if not distributed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    print(\"Batch size is:\", args.batch_size, \"epochs\", args.epochs)\n",
    "\n",
    "    #############\n",
    "    # Create MONAI dataset\n",
    "    training_list = load_decathlon_datalist(\n",
    "        data_list_file_path=args.dataset_json,\n",
    "        data_list_key=\"training\",\n",
    "        base_dir=args.data_root,\n",
    "    )\n",
    "    validation_list = load_decathlon_datalist(\n",
    "        data_list_file_path=args.dataset_json,\n",
    "        data_list_key=\"validation\",\n",
    "        base_dir=args.data_root,\n",
    "    )\n",
    "\n",
    "    if args.quick:  # for debugging on a small subset\n",
    "        training_list = training_list[:16]\n",
    "        validation_list = validation_list[:16]\n",
    "\n",
    "    train_transform = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\"], reader=WSIReader, backend=\"cucim\", dtype=np.uint8, level=1, image_only=True),\n",
    "            LabelEncodeIntegerGraded(keys=[\"label\"], num_classes=args.num_classes),\n",
    "            RandGridPatchd(\n",
    "                keys=[\"image\"],\n",
    "                patch_size=(args.tile_size, args.tile_size),\n",
    "                num_patches=args.tile_count,\n",
    "                sort_fn=\"min\",\n",
    "                pad_mode=None,\n",
    "                constant_values=255,\n",
    "            ),\n",
    "            SplitDimd(keys=[\"image\"], dim=0, keepdim=False, list_output=True),\n",
    "            RandFlipd(keys=[\"image\"], spatial_axis=0, prob=0.5),\n",
    "            RandFlipd(keys=[\"image\"], spatial_axis=1, prob=0.5),\n",
    "            RandRotate90d(keys=[\"image\"], prob=0.5),\n",
    "            ScaleIntensityRanged(keys=[\"image\"], a_min=np.float32(0), a_max=np.float32(255)),\n",
    "            ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    valid_transform = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\"], reader=WSIReader, backend=\"cucim\", dtype=np.uint8, level=1, image_only=True),\n",
    "            LabelEncodeIntegerGraded(keys=[\"label\"], num_classes=args.num_classes),\n",
    "            GridPatchd(\n",
    "                keys=[\"image\"],\n",
    "                patch_size=(args.tile_size, args.tile_size),\n",
    "                threshold=0.999 * 3 * 255 * args.tile_size * args.tile_size,\n",
    "                pad_mode=None,\n",
    "                constant_values=255,\n",
    "            ),\n",
    "            SplitDimd(keys=[\"image\"], dim=0, keepdim=False, list_output=True),\n",
    "            ScaleIntensityRanged(keys=[\"image\"], a_min=np.float32(0), a_max=np.float32(255)),\n",
    "            ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset_train = Dataset(data=training_list, transform=train_transform)\n",
    "    dataset_valid = Dataset(data=validation_list, transform=valid_transform)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset_train,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=list_data_collate,\n",
    "    )\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "        dataset_valid,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True,\n",
    "        collate_fn=list_data_collate,\n",
    "    )\n",
    "\n",
    "    print(\"Dataset training:\", len(dataset_train), \"validation:\", len(dataset_valid))\n",
    "\n",
    "    model = milmodel.MILModel(num_classes=args.num_classes, pretrained=True, mil_mode=args.mil_mode)\n",
    "\n",
    "    best_acc = 0\n",
    "    start_epoch = 0\n",
    "\n",
    "    model.cuda(0)\n",
    "    params = model.parameters()\n",
    "    m = model \n",
    "    params = [\n",
    "        {\"params\": list(m.attention.parameters()) + list(m.myfc.parameters()) + list(m.net.parameters())},\n",
    "        {\"params\": list(m.transformer.parameters()), \"lr\": 6e-6, \"weight_decay\": 0.1},\n",
    "    ]\n",
    "\n",
    "    optimizer = torch.optim.AdamW(params, lr=args.optim_lr, weight_decay=args.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=0)\n",
    "\n",
    "    # RUN TRAINING\n",
    "    n_epochs = args.epochs\n",
    "    val_acc_max = 0.0\n",
    "\n",
    "    scaler = GradScaler(enabled=args.amp)\n",
    "\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "\n",
    "        print(time.ctime(), \"Epoch:\", epoch)\n",
    "\n",
    "        epoch_time = time.time()\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, scaler=scaler, epoch=epoch, epochs=n_epochs)\n",
    "\n",
    "        print(\n",
    "            \"Final training  {}/{}\".format(epoch, n_epochs - 1),\n",
    "            \"loss: {:.4f}\".format(train_loss),\n",
    "            \"acc: {:.4f}\".format(train_acc),\n",
    "            \"time {:.2f}s\".format(time.time() - epoch_time),\n",
    "        )\n",
    "\n",
    "\n",
    "        b_new_best = False\n",
    "        val_acc = 0\n",
    "        if (epoch + 1) % args.val_every == 0:\n",
    "            epoch_time = time.time()\n",
    "            val_loss, val_acc, qwk = val_epoch(model, valid_loader, epoch=epoch, epochs=n_epochs, max_tiles=args.tile_count)\n",
    "\n",
    "            print(\n",
    "                \"Final validation  {}/{}\".format(epoch, n_epochs - 1),\n",
    "                \"loss: {:.4f}\".format(val_loss),\n",
    "                \"acc: {:.4f}\".format(val_acc),\n",
    "                \"qwk: {:.4f}\".format(qwk),\n",
    "                \"time {:.2f}s\".format(time.time() - epoch_time),\n",
    "            )\n",
    "               \n",
    "            val_acc = qwk\n",
    "\n",
    "            if val_acc > val_acc_max:\n",
    "                print(\"qwk ({:.6f} --> {:.6f})\".format(val_acc_max, val_acc))\n",
    "                val_acc_max = val_acc\n",
    "                print(\"New best model!\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    print(\"ALL DONE\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Multiple Instance Learning (MIL) example of classification from WSI.\")\n",
    "    parser.add_argument(\n",
    "        \"--data_root\", default=\"/PandaChallenge2020/train_images/\", help=\"path to root folder of images\"\n",
    "    )\n",
    "    parser.add_argument(\"--dataset_json\", default=None, type=str, help=\"path to dataset json file\")\n",
    "\n",
    "    parser.add_argument(\"--num_classes\", default=5, type=int, help=\"number of output classes\")\n",
    "    parser.add_argument(\"--mil_mode\", default=\"att_trans\", help=\"MIL algorithm\")\n",
    "    parser.add_argument(\n",
    "        \"--tile_count\", default=44, type=int, help=\"number of patches (instances) to extract from WSI image\"\n",
    "    )\n",
    "    parser.add_argument(\"--tile_size\", default=256, type=int, help=\"size of square patch (instance) in pixels\")\n",
    "\n",
    "    parser.add_argument(\"--checkpoint\", default=None, help=\"load existing checkpoint\")\n",
    "    parser.add_argument(\n",
    "        \"--validate\",\n",
    "        action=\"store_true\",\n",
    "        help=\"run only inference on the validation set, must specify the checkpoint argument\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--logdir\", default=None, help=\"path to log directory to store Tensorboard logs\")\n",
    "\n",
    "    parser.add_argument(\"--epochs\", \"--max_epochs\", default=50, type=int, help=\"number of training epochs\")\n",
    "    parser.add_argument(\"--batch_size\", default=4, type=int, help=\"batch size, the number of WSI images per gpu\")\n",
    "    parser.add_argument(\"--optim_lr\", default=3e-5, type=float, help=\"initial learning rate\")\n",
    "\n",
    "    parser.add_argument(\"--weight_decay\", default=0, type=float, help=\"optimizer weight decay\")\n",
    "    parser.add_argument(\"--amp\", action=\"store_true\", help=\"use AMP, recommended\")\n",
    "    parser.add_argument(\n",
    "        \"--val_every\",\n",
    "        \"--val_interval\",\n",
    "        default=1,\n",
    "        type=int,\n",
    "        help=\"run validation after this number of epochs, default 1 to run every epoch\",\n",
    "    )\n",
    "    parser.add_argument(\"--workers\", default=2, type=int, help=\"number of workers for data loading\")\n",
    "\n",
    "    parser.add_argument(\"--quick\", action=\"store_true\", help=\"use a small subset of data for debugging\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(\"Argument values:\")\n",
    "    for k, v in vars(args).items():\n",
    "        print(k, \"=>\", v)\n",
    "    print(\"-----------------\")\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "\n",
    "    if args.dataset_json is None:\n",
    "        # download default json datalist\n",
    "        resource = \"https://drive.google.com/uc?id=1L6PtKBlHHyUgTE4rVhRuOLTQKgD4tBRK\"\n",
    "        dst = \"./datalist_panda_0.json\"\n",
    "        if not os.path.exists(dst):\n",
    "            gdown.download(resource, dst, quiet=False)\n",
    "        args.dataset_json = dst\n",
    "\n",
    "    main_worker(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b05a272c-9aeb-469a-8ca1-6c8c51dbb4de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argument values:\n",
      "data_root => /Panda/train_images\n",
      "dataset_json => None\n",
      "num_classes => 5\n",
      "mil_mode => att_trans\n",
      "tile_count => 44\n",
      "tile_size => 256\n",
      "checkpoint => None\n",
      "validate => False\n",
      "logdir => None\n",
      "epochs => 1\n",
      "batch_size => 4\n",
      "optim_lr => 3e-05\n",
      "weight_decay => 0\n",
      "amp => True\n",
      "val_every => 1\n",
      "workers => 2\n",
      "quick => False\n",
      "-----------------\n",
      "Batch size is: 4 epochs 1\n",
      "Dataset training: 100 validation: 20\n",
      "WARNING:py.warnings:The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "\n",
      "WARNING:py.warnings:Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [01:31<00:00, 1.13MB/s]\n",
      "WARNING:py.warnings:enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "\n",
      "WARNING:py.warnings:`torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "\n",
      "Thu Feb  6 15:32:41 2025 Epoch: 0\n",
      "WARNING:py.warnings:`torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "\n",
      "Epoch 0/1 0/25 loss: 0.5229 acc: 0.0000 time 9.63s\n",
      "Epoch 0/1 1/25 loss: 0.8195 acc: 0.0000 time 0.28s\n",
      "Epoch 0/1 2/25 loss: 0.8265 acc: 0.0000 time 0.28s\n",
      "Epoch 0/1 3/25 loss: 0.7966 acc: 0.0000 time 0.28s\n",
      "Epoch 0/1 4/25 loss: 0.7389 acc: 0.1000 time 1.74s\n",
      "Epoch 0/1 5/25 loss: 0.7208 acc: 0.0833 time 2.02s\n",
      "Epoch 0/1 6/25 loss: 0.7102 acc: 0.0714 time 1.65s\n",
      "Epoch 0/1 7/25 loss: 0.6722 acc: 0.0938 time 2.58s\n",
      "Epoch 0/1 8/25 loss: 0.6698 acc: 0.1111 time 0.28s\n",
      "Epoch 0/1 9/25 loss: 0.6458 acc: 0.1250 time 2.34s\n",
      "Epoch 0/1 10/25 loss: 0.6539 acc: 0.1136 time 0.52s\n",
      "Epoch 0/1 11/25 loss: 0.6435 acc: 0.1042 time 3.92s\n",
      "Epoch 0/1 12/25 loss: 0.6328 acc: 0.0962 time 0.28s\n",
      "Epoch 0/1 13/25 loss: 0.6333 acc: 0.1071 time 4.06s\n",
      "Epoch 0/1 14/25 loss: 0.6035 acc: 0.1000 time 0.28s\n",
      "Epoch 0/1 15/25 loss: 0.6010 acc: 0.0938 time 2.86s\n",
      "Epoch 0/1 16/25 loss: 0.5793 acc: 0.0882 time 0.28s\n",
      "Epoch 0/1 17/25 loss: 0.5819 acc: 0.0833 time 2.29s\n",
      "Epoch 0/1 18/25 loss: 0.5630 acc: 0.1184 time 2.33s\n",
      "Epoch 0/1 19/25 loss: 0.5563 acc: 0.1125 time 0.82s\n",
      "Epoch 0/1 20/25 loss: 0.5540 acc: 0.1190 time 2.82s\n",
      "Epoch 0/1 21/25 loss: 0.5649 acc: 0.1136 time 0.28s\n",
      "Epoch 0/1 22/25 loss: 0.5711 acc: 0.1087 time 2.23s\n",
      "Epoch 0/1 23/25 loss: 0.5729 acc: 0.1146 time 0.29s\n",
      "Epoch 0/1 24/25 loss: 0.5722 acc: 0.1200 time 1.80s\n",
      "Final training  0/0 loss: 0.5722 acc: 0.1200 time 46.19s\n",
      "Val epoch 0/1 0/20 loss: 0.1259 acc: 0.0000 time 1.24s\n",
      "Val epoch 0/1 1/20 loss: 0.1394 acc: 0.5000 time 0.25s\n",
      "Val epoch 0/1 2/20 loss: 0.1998 acc: 0.3333 time 0.22s\n",
      "Val epoch 0/1 3/20 loss: 0.3459 acc: 0.2500 time 0.53s\n",
      "Val epoch 0/1 4/20 loss: 0.3062 acc: 0.4000 time 0.26s\n",
      "Val epoch 0/1 5/20 loss: 0.3098 acc: 0.5000 time 0.31s\n",
      "Val epoch 0/1 6/20 loss: 0.4512 acc: 0.4286 time 1.26s\n",
      "Val epoch 0/1 7/20 loss: 0.4374 acc: 0.3750 time 0.22s\n",
      "Val epoch 0/1 8/20 loss: 0.4072 acc: 0.4444 time 0.80s\n",
      "Val epoch 0/1 9/20 loss: 0.4471 acc: 0.4000 time 0.22s\n",
      "Val epoch 0/1 10/20 loss: 0.4325 acc: 0.4545 time 0.44s\n",
      "Val epoch 0/1 11/20 loss: 0.4130 acc: 0.5000 time 0.04s\n",
      "Val epoch 0/1 12/20 loss: 0.3903 acc: 0.5385 time 0.88s\n",
      "Val epoch 0/1 13/20 loss: 0.3746 acc: 0.5000 time 0.28s\n",
      "Val epoch 0/1 14/20 loss: 0.3707 acc: 0.4667 time 0.40s\n",
      "Val epoch 0/1 15/20 loss: 0.3580 acc: 0.5000 time 0.03s\n",
      "Val epoch 0/1 16/20 loss: 0.3912 acc: 0.4706 time 0.74s\n",
      "Val epoch 0/1 17/20 loss: 0.3812 acc: 0.5000 time 0.71s\n",
      "Val epoch 0/1 18/20 loss: 0.3736 acc: 0.4737 time 0.20s\n",
      "Val epoch 0/1 19/20 loss: 0.4200 acc: 0.4500 time 1.36s\n",
      "Final validation  0/0 loss: 0.4200 acc: 0.4500 qwk: 0.3515 time 10.52s\n",
      "qwk (0.000000 --> 0.351464)\n",
      "New best model!\n",
      "ALL DONE\n",
      "Generating '/tmp/nsys-report-9083.qdstrm'\n",
      "[1/8] [========================100%] profile_output.nsys-rep\n",
      "[2/8] [========================100%] profile_output.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)       Med (ns)      Min (ns)     Max (ns)    StdDev (ns)   Style             Range          \n",
      " --------  ---------------  ---------  -------------  -------------  -----------  -----------  -----------  -------  -------------------------\n",
      "     59.3      46193622717          1  46193622717.0  46193622717.0  46193622717  46193622717          0.0  PushPop  :train_epoch             \n",
      "     13.6      10586311644         25    423452465.8    277467562.0    275170111   3923858650  729257770.1  PushPop  :Start batch             \n",
      "     13.5      10524538603          1  10524538603.0  10524538603.0  10524538603  10524538603          0.0  PushPop  :val_epoch               \n",
      "      9.3       7204685111         25    288187404.4    200035022.0    196075981   2245100770  407822174.8  PushPop  :back_prop               \n",
      "      3.5       2759418310         25    110376732.4     53434940.0     29757480   1640725765  319012234.0  PushPop  :do inference            \n",
      "      0.4        291285080         25     11651403.2     11678612.0     11348786     12863208     304420.7  PushPop  :get data                \n",
      "      0.4        288923137         25     11556925.5     11114655.0     10756757     23103644    2409390.0  PushPop  :metrics                 \n",
      "      0.0         37674677         25      1506987.1      1638757.0       742113      3376820     641168.9  PushPop  :zero optimizer          \n",
      "      0.0            72641          2        36320.5        36320.5        24550        48091      16646.0  PushPop  :LabelEncodeIntegerGraded\n",
      "\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)       Med (ns)     Min (ns)    Max (ns)     StdDev (ns)            Name         \n",
      " --------  ---------------  ---------  -------------  -------------  --------  ------------  -------------  ----------------------\n",
      "     94.1   14274058854088        191  74733292429.8  98958128732.0    209032  151568693029  47813688248.9  pthread_cond_wait     \n",
      "      2.0     305589948134       3084     99088828.8    100256128.0      3535     453151544     12624517.0  poll                  \n",
      "      1.6     237559245573      91106      2607503.8       561192.0      1000  100148449353    359476981.1  read                  \n",
      "      0.9     142668404690     536030       266157.5       266723.0    207917       8138097        17212.0  usleep                \n",
      "      0.8     117611878443         69   1704519977.4    773533270.0    113206    9278831820   2064419342.8  sem_wait              \n",
      "      0.6      90680798978        542    167307747.2       105179.5      2707     520127203    236042376.9  pthread_cond_timedwait\n",
      "      0.0       2566699712        149     17226172.6        59940.0      1068     401013752     75682513.2  pthread_mutex_lock    \n",
      "      0.0       1415677770       4598       307889.9        35524.5      1001     183486994      4944287.1  ioctl                 \n",
      "      0.0        386924976          4     96731244.0     71174367.0  60724839     183851403     58545795.2  fork                  \n",
      "      0.0        277967685          4     69491921.3     67945206.0      3171     142074102     71670793.7  pthread_rwlock_wrlock \n",
      "      0.0        221770633       2330        95180.5         9770.5      1004       4779611       179901.5  write                 \n",
      "      0.0        109851442        197       557621.5        33888.0     14957      22287381      2235110.0  pthread_join          \n",
      "      0.0        109823067      38090         2883.3         2470.0      1003        105874         1863.1  stat64                \n",
      "      0.0         75191899      12144         6191.7         4717.0      1054       8878324        81933.3  open64                \n",
      "      0.0         71686343      40149         1785.5         1738.0      1000         21258          520.7  lstat64               \n",
      "      0.0         68079414        108       630364.9       412850.0    187461      12467165      1393292.1  pthread_create        \n",
      "      0.0         47045556          8      5880694.5       959572.5      3990      26852289     10070765.9  futex                 \n",
      "      0.0         45497574        182       249986.7        20595.0      9145      41267947      3057261.7  connect               \n",
      "      0.0         35283142         14      2520224.4      2085549.0   2073451       5118427      1099517.6  nanosleep             \n",
      "      0.0         20339787        487        41765.5        16578.0      4642       3544957       236541.3  mmap64                \n",
      "      0.0         14130054        180        78500.3        61931.0      9250        236894        42343.2  recvmsg               \n",
      "      0.0          5083582         65        78209.0        80193.0     17399        137638        12683.0  sleep                 \n",
      "      0.0          3864226         21       184010.8       202061.0     92866        286666        61352.7  sem_timedwait         \n",
      "      0.0          3545361        635         5583.2         3586.0      1078        136163         9336.2  pthread_cond_signal   \n",
      "      0.0          3499088        183        19120.7        17289.0      6052         67136         7573.4  socket                \n",
      "      0.0          3337655       2221         1502.8         1194.0      1000         29503         1179.1  fstat64               \n",
      "      0.0          2901257        115        25228.3        39839.0      1079         71931        19949.3  fgets                 \n",
      "      0.0          2440935         63        38745.0        28440.0     14349        467532        57985.7  mmap                  \n",
      "      0.0           403411          6        67235.2        37869.0      2443        181312        75642.0  waitpid               \n",
      "      0.0           367864         36        10218.4         2578.0      1429         71574        16958.2  fopen                 \n",
      "      0.0           267590        187         1431.0         1385.0      1004          2460          241.2  dup                   \n",
      "      0.0           163034          2        81517.0        81517.0     18114        144920        89665.4  backtrace             \n",
      "      0.0           147259         22         6693.6         3992.0      1073         20250         6031.6  fclose                \n",
      "      0.0           135517          4        33879.3        22562.0     14048         76345        29258.1  stat                  \n",
      "      0.0           131923         15         8794.9         6019.0      3336         15924         4955.0  pipe2                 \n",
      "      0.0           126715          9        14079.4        10935.0      4421         25374         7418.9  fopen64               \n",
      "      0.0           106645         67         1591.7         1566.0      1050          2587          344.8  fcntl                 \n",
      "      0.0            92891          2        46445.5        46445.5     31414         61477        21257.8  getdelim              \n",
      "      0.0            87038          9         9670.9         7090.0      1696         30180         8189.7  open                  \n",
      "      0.0            74957          9         8328.6         6572.0      4818         20170         4705.7  pthread_cond_broadcast\n",
      "      0.0            26430          4         6607.5         2622.0      2463         18723         8078.1  sigaction             \n",
      "      0.0            26255          7         3750.7         2374.0      1314         12189         3838.1  waitid                \n",
      "      0.0            15829          8         1978.6         1675.5      1019          3184          927.4  pthread_mutex_trylock \n",
      "      0.0            13141          2         6570.5         6570.5      5929          7212          907.2  fwrite                \n",
      "      0.0            11366          6         1894.3         1511.0      1225          4065         1077.1  fflush                \n",
      "      0.0            10394          1        10394.0        10394.0     10394         10394            0.0  bind                  \n",
      "      0.0             7422          1         7422.0         7422.0      7422          7422            0.0  openat64              \n",
      "      0.0             5636          1         5636.0         5636.0      5636          5636            0.0  fstatat64             \n",
      "      0.0             4198          1         4198.0         4198.0      4198          4198            0.0  fread                 \n",
      "      0.0             1703          1         1703.0         1703.0      1703          1703            0.0  getc                  \n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)               Name             \n",
      " --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  ------------------------------\n",
      "     35.6       3390928001        579   5856525.0     3739.0      2410  151845197   25352641.3  cudaStreamSynchronize         \n",
      "     14.7       1402431485      64245     21829.4    10643.0      3805   78112238     532396.4  cudaLaunchKernel              \n",
      "     13.8       1311702755       5302    247397.7    90816.0      3206   31375478     663855.4  cudaDeviceSynchronize         \n",
      "     13.3       1263842920       4820    262208.1   102964.0      1976    7002974     493453.1  cudaEventSynchronize          \n",
      "      7.0        662281502         11  60207409.3   142489.0     49583  193475559   84896713.2  cudaHostAlloc                 \n",
      "      4.6        435526473        956    455571.6    25561.0      7009   34601298    2169786.3  cudaMemcpyAsync               \n",
      "      3.7        352857705      21381     16503.3     3959.0      1297  142120211    1269861.2  cudaEventRecord               \n",
      "      2.2        205396789        458    448464.6   294442.5      6735    2559264     339463.3  cudaMalloc                    \n",
      "      1.3        120815436        364    331910.5   255980.0      1081    3309709     286143.4  cudaFree                      \n",
      "      1.1        102335776         24   4263990.7  4275507.5   1191015    9025453    2718550.8  cuLibraryLoadData             \n",
      "      1.0         99351928       7460     13318.0     8070.0      4276    2815195      88624.9  cudaLaunchKernelExC_v11060    \n",
      "      0.6         53357688        110    485069.9   102073.0     40143   15777122    1674354.3  cuLibraryUnload               \n",
      "      0.3         30566942       3251      9402.3     7705.0       161      80030       8549.8  cudaMemsetAsync               \n",
      "      0.3         26762074        482     55523.0    50901.0     34699     184327      17384.6  cudaMemGetInfo                \n",
      "      0.2         18395893        920     19995.5     1348.0       306    9992272     348354.9  cuKernelGetFunction           \n",
      "      0.2         15510655      12396      1251.3     1152.0       399      42126       1187.9  cudaStreamIsCapturing_v10000  \n",
      "      0.1         13250354        920     14402.6    11813.0      4788      89786       8716.6  cuLaunchKernel                \n",
      "      0.0          3002670        964      3114.8     3615.5       708      54329       2668.4  cudaEventCreate               \n",
      "      0.0          1349818        964      1400.2     1481.0       608      13858        834.5  cudaEventDestroy              \n",
      "      0.0          1056280         32     33008.8    10120.0      2774     345673      69225.9  cudaStreamCreateWithFlags     \n",
      "      0.0           697687       2046       341.0      266.0       115      20512        666.2  cuGetProcAddress_v2           \n",
      "      0.0           314970         36      8749.2     8224.0      1280      24828       4595.0  cuLibraryGetKernel            \n",
      "      0.0            65657         38      1727.8     1000.0       422      10596       2298.7  cudaEventCreateWithFlags      \n",
      "      0.0            30350          8      3793.8     2550.0      2054       9304       2474.7  cuInit                        \n",
      "      0.0             3163          6       527.2      282.0       235       1678        568.1  cuModuleGetLoadingMode        \n",
      "      0.0             2776          2      1388.0     1388.0       793       1983        841.5  cudaGetDriverEntryPoint_v11030\n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)   Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                                                  Name                                                \n",
      " --------  ---------------  ---------  ---------  ---------  --------  --------  -----------  ----------------------------------------------------------------------------------------------------\n",
      "     15.8       1532895047      16311    93979.2    24640.0      1600   1160230     182197.3  void cudnn::engines_precompiled::nchwToNhwcKernel<__half, __half, float, (bool)0, (bool)1, (cudnnKe…\n",
      "      6.8        657521235       9635    68243.0    22688.0      1664   1144870     146282.7  void cudnn::engines_precompiled::nhwcToNchwKernel<__half, __half, float, (bool)1, (bool)0, (cudnnKe…\n",
      "      6.6        640734510        300  2135781.7  1269431.0    993510   4326456    1266135.3  void cudnn::bn_bw_1C11_kernel_new<__half, float, float2, (int)512, (bool)1, (int)1>(T2, T2, T2, T2,…\n",
      "      6.5        635198922       1621   391856.2   205217.0      1504   1753930     468928.8  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctor_add<c10::Half>, at::…\n",
      "      5.0        487212042       1325   367707.2   204641.0      4639   1744170     420882.1  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<c10::Half, c10::Ha…\n",
      "      4.2        407854906       3169   128701.5    60480.0      1247   1164517     214177.5  void at::native::vectorized_elementwise_kernel<(int)4, at::native::<unnamed>::launch_clamp_scalar(a…\n",
      "      3.8        371221037        300  1237403.5   727396.0    551971   2497709     755953.8  void cudnn::bn_fw_tr_1C11_kernel_NCHW<__half, float, int, (int)512, (bool)1, (int)1, (bool)1>(cudnn…\n",
      "      2.8        274726237        325   845311.5   372674.0    364706   1614345     543825.9  void cudnn::bn_bw_1C11_kernel_new<__half, float, float2, (int)128, (bool)1, (int)1>(T2, T2, T2, T2,…\n",
      "      2.8        273545450       1604   170539.6    70768.5      9664    870788     216023.8  ampere_fp16_s1688gemm_fp16_256x64_ldg8_f2f_stages_32x1_nn                                           \n",
      "      1.9        188629616        475   397115.0   209153.0    201410    750213     235422.8  void cudnn::bn_bw_1C11_kernel_new<__half, float, float2, (int)32, (bool)1, (int)1>(T2, T2, T2, T2, …\n",
      "      1.6        159594796        325   491060.9   217857.0    213185    946693     314908.8  void cudnn::bn_fw_tr_1C11_kernel_NCHW<__half, float, int, (int)128, (bool)1, (int)1, (bool)1>(cudnn…\n",
      "      1.6        157775542        330   478107.7   435346.0    234529    992613     244186.7  ampere_fp16_s1688gemm_fp16_256x64_ldg8_f2f_stages_32x1_nt                                           \n",
      "      1.5        142169618        933   152379.0    83585.0      5375    473539     147542.4  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_128x128_32x3_nhwc_ali…\n",
      "      1.1        106672656        301   354394.2   397474.0    193697    818948     106302.8  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_128x128_32x3_nhwc_uni…\n",
      "      1.1        103621511        175   592122.9   519875.0    236386   3570899     356757.6  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_war…\n",
      "      1.1        103189755       1908    54082.7    31568.5      2080    299202      65128.1  void cudnn::bn_fw_inf_1C11_kernel_NCHW<__half, float, (bool)1, (int)1>(T2, T2, cudnnTensorStruct, c…\n",
      "      1.0         98804389         25  3952175.6  3948118.0   3846805   4048881      42428.3  void at::native::<unnamed>::max_pool_backward_nchw<c10::Half, float>(const T1 *, const long *, int,…\n",
      "      1.0         95856186        680   140965.0    76864.5     20576    510370     131628.4  ampere_fp16_s1688gemm_fp16_64x128_sliced1x2_ldg8_f2f_nn                                             \n",
      "      1.0         93219523        611   152568.8    94529.0      4800   4809209     315890.4  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_128x64_32x6_nhwc_alig…\n",
      "      0.8         80931051        137   590737.6   677060.0    276481    730948     168435.9  ampere_s16816gemm_fp16_64x64_ldg8_stages_64x5_tn                                                    \n",
      "      0.8         79612618        107   744043.2   713700.0    324802   2713487     386827.6  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warp…\n",
      "      0.8         78828311        137   575389.1   662244.0    269121    707780     165427.6  void cutlass::Kernel2<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_32x1_nt_align8>(T1::Params)\n",
      "      0.8         78069850        322   242453.0   223265.0    195169    884517      81618.6  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_war…\n",
      "      0.8         76019107        169   449817.2   465762.0    214561    496515      54696.9  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warp…\n",
      "      0.8         75272837        126   597403.5   515490.5    227490   1072838     175007.2  sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_war…\n",
      "      0.8         74550272        225   331334.5   424738.0     83233    482082     157277.0  void at::native::<unnamed>::multi_tensor_apply_kernel<at::native::<unnamed>::TensorListScalarListMe…\n",
      "      0.8         73369853       4786    15330.1     4320.0      1536    301825      30531.5  void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIterator…\n",
      "      0.7         72244362        475   152093.4    74720.0     72448    296770      95645.2  void cudnn::bn_fw_tr_1C11_singleread_spec<__half2, (int)512, (int)1, (int)2, (int)20>(cudnn::bn_fw_…\n",
      "      0.7         71009761        243   292221.2   259394.0    241185    503363      79658.8  ampere_fp16_s1688gemm_fp16_64x128_sliced1x2_ldg8_f2f_nt                                             \n",
      "      0.7         68688963        300   228963.2   244801.0    117120    254690      35631.9  void at::native::<unnamed>::multi_tensor_apply_kernel<at::native::<unnamed>::TensorListMetadata<(in…\n",
      "      0.7         63368465        201   315266.0   242144.0    218625    481187     106312.2  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816wgrad_optimized_f16_256x64_32x4_nhwc_alig…\n",
      "      0.6         59212701        270   219306.3   110432.0     13504    557283     211202.1  sm86_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warpsize2x2x…\n",
      "      0.6         58906916        445   132375.1    84673.0      6176    820900     122030.0  sm86_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage3_warpsize2x2…\n",
      "      0.6         57715540        225   256513.5   327938.0     61952    376898     122579.4  void at::native::<unnamed>::multi_tensor_apply_kernel<at::native::<unnamed>::TensorListMetadata<(in…\n",
      "      0.6         57607532        200   288037.7   347345.0     96480    376769     107127.1  void at::native::<unnamed>::multi_tensor_apply_kernel<at::native::<unnamed>::TensorListMetadata<(in…\n",
      "      0.6         57230327        574    99704.4    62672.0      5248    819429     123038.3  sm86_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warpsize2x2x…\n",
      "      0.6         54916367        189   290562.8   170657.0      3744    706212     255685.4  void cutlass::Kernel2<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_32x1_nn_align8>(T1::Params)\n",
      "      0.5         51544905         70   736355.8   665492.0    312642   3511603     491046.9  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warps…\n",
      "      0.5         49746666        106   469308.2   466498.0    460770    505955      10461.8  ampere_s1688gemm_fp16_128x128_ldg8_stages_32x1_tn                                                   \n",
      "      0.5         49274014        466   105738.2    72400.5      6048    466082     115198.6  sm86_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage3_warpsize2x2…\n",
      "      0.5         46652710        611    76354.7    55488.0      5504    814788      85061.9  sm86_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage4_warpsize2x2x…\n",
      "      0.5         45058194         78   577669.2   317170.0     81121   5674397     887734.2  _5x_cudnn_ampere_fp16_scudnn_fp16_128x128_relu_medium_nn_v1                                         \n",
      "      0.5         43885252         81   541793.2   549538.0    276705    575076      44850.3  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_128x64_32x6_nhwc_unit…\n",
      "      0.4         43219641        339   127491.6    84768.0     14176    606275     126188.6  sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2…\n",
      "      0.4         41236323        264   156198.2   104609.0     23872    372834     131900.3  ampere_fp16_s16816gemm_fp16_256x128_ldg8_f2f_stages_32x3_nn                                         \n",
      "      0.4         40660118         91   446814.5   457954.0    224833    476483      48320.7  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816wgrad_optimized_f16_256x128_32x3_nhwc_ali…\n",
      "      0.4         40242854        511    78753.1    55712.0      6079    813668      88808.1  sm86_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage4_warp…\n",
      "      0.4         39416278         61   646168.5   316194.0      8576   1271686     522763.3  void at::native::<unnamed>::max_pool_forward_nchw<c10::Half, c10::Half>(int, const T1 *, long, long…\n",
      "      0.4         38992945        200   194964.7   235777.0     23105    253761      79646.4  void at::native::<unnamed>::multi_tensor_apply_kernel<at::native::<unnamed>::TensorListScalarListMe…\n",
      "      0.4         38852037        200   194260.2   233937.5     67904    257026      71175.3  void at::native::<unnamed>::multi_tensor_apply_kernel<at::native::<unnamed>::TensorListMetadata<(in…\n",
      "      0.4         38673549        175   220991.7   245089.0    113440    263713      45355.4  void at::native::<unnamed>::multi_tensor_apply_kernel<at::native::<unnamed>::TensorListMetadata<(in…\n",
      "      0.4         38545863        175   220262.1   244385.0    117696    255425      43580.5  void at::native::<unnamed>::multi_tensor_apply_kernel<at::native::<unnamed>::TensorListMetadata<(in…\n",
      "      0.4         37907620         68   557465.0   526083.0    401858   1081318     153596.8  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_128x128_32x3_nhwc_ali…\n",
      "      0.4         37161417        106   350579.4   350082.0    344514    366946       4026.5  ampere_fp16_s16816gemm_fp16_256x128_ldg8_f2f_stages_32x3_nt                                         \n",
      "      0.4         36082260        141   255902.6   157345.0     15168   2952943     385443.1  _5x_cudnn_ampere_fp16_scudnn_fp16_128x64_relu_medium_nn_v1                                          \n",
      "      0.4         36038194         81   444916.0   261922.0      7904   4625592     740095.7  void implicit_convolve_sgemm<__half, __half, (int)1024, (int)5, (int)5, (int)3, (int)3, (int)3, (in…\n",
      "      0.4         34667240       2660    13032.8     3809.0      1376    113856      23086.1  void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIterator…\n",
      "      0.4         34207059         79   433000.7   431490.0    425474    461315       6338.8  ampere_s16816gemm_fp16_256x128_ldg8_stages_32x3_tn                                                  \n",
      "      0.3         31295833         53   590487.4   370914.0      7936   4765465     903707.5  void implicit_convolve_sgemm<__half, __half, (int)128, (int)5, (int)5, (int)3, (int)3, (int)3, (int…\n",
      "      0.3         28713652        225   127616.2    59168.0     57664    222497      77366.4  void cudnn::bn_bw_1C11_singleread_spec<__half2, (int)512, (int)1, (int)2, (int)7>(cudnn::bn_bw_1C11…\n",
      "      0.3         27862589         93   299597.7   191265.0     23584    764995     275872.5  sm80_xmma_fprop_image_first_layer_f16f16_f32_f16_nhwckrsc_nhwc_hmma_k64c4r7s7_stride2x2_tile16x64x6…\n",
      "      0.3         27752811        230   120664.4    94304.5      6848    810277     117314.6  sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x…\n",
      "      0.3         27196168         64   424940.1   418418.0    271969    695524      62166.4  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x128_32x6_nhwc_unit…\n",
      "      0.3         26114554        268    97442.4    75600.5      6624    569091      96274.1  sm86_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage4_warpsize4x1x…\n",
      "      0.3         25981996        277    93797.8    79553.0      9952    610563      94875.6  sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_warpsize2x2…\n",
      "      0.3         25341290        221   114666.5    76256.0      4672    342242     130648.0  ampere_fp16_s16816gemm_fp16_64x64_ldg8_f2f_stages_64x5_nn                                           \n",
      "      0.3         24664828        365    67574.9    70529.0      7648    533891      75872.1  void cudnn::cnn::reduce_wgrad_nchw_helper<float, __half>(void *, const void *, T1, int, int)        \n",
      "      0.3         24502021         51   480431.8   426754.0    210049    943206     150892.4  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816wgrad_optimized_f16_128x128_32x3_nhwc_ali…\n",
      "      0.2         22791352         27   844124.1   839685.0    835043    912421      17308.7  sm80_xmma_wgrad_image_first_layer_f16f16_f32_f32_nhwckrsc_nhwc_hmma_k64c4r7s7_stride2x2_tile16x64x6…\n",
      "      0.2         21771548        203   107249.0    64960.0     15168    813444     118174.1  sm86_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x128x64_stage2_warpsize4x2…\n",
      "      0.2         21709620         30   723654.0   518178.0    260866   3260113     719657.2  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warp…\n",
      "      0.2         21204542        299    70918.2    53569.0      4800    706084      84370.5  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_64x64_64x5_nhwc_align…\n",
      "      0.2         19356107        235    82366.4    48768.0      4928    598340     114953.4  sm86_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage4_warp…\n",
      "      0.2         19071882         40   476797.1   332962.0     26464   2908879     602171.2  _5x_cudnn_ampere_fp16_scudnn_fp16_128x64_relu_xregs_large_nn_v1                                     \n",
      "      0.2         19040108        225    84622.7    39872.0     35136    149089      52004.7  void cudnn::bn_fw_tr_1C11_singleread_spec<__half2, (int)512, (int)1, (int)2, (int)10>(cudnn::bn_fw_…\n",
      "      0.2         18982006        256    74148.5    69296.0      7104    170177      46510.8  void cutlass::Kernel2<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_64x1_nn_align8>(T1::Params)\n",
      "      0.2         18938659         43   440433.9   465411.0    227169    479651      69289.8  void cutlass__5x_cudnn::Kernel<cutlass__5x_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass__5x…\n",
      "      0.2         18829454        165   114117.9    80640.0     14752    501187     107928.4  sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_warpsize4x2…\n",
      "      0.2         18373153         31   592682.4   588739.0    576932    617412      11796.6  ampere_s1688gemm_fp16_64x128_sliced1x2_ldg8_tn                                                      \n",
      "      0.2         18201512         38   478987.2   339682.0     21696   2956079     616216.9  sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x16x32_sta…\n",
      "      0.2         17035652        202    84334.9    58704.5      5184    801636     115453.9  sm86_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage3_warpsize2x2x1…\n",
      "      0.2         16389501        121   135450.4   100577.0     18016    667395     122418.8  sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_warpsize2x4…\n",
      "      0.2         16305656         18   905869.8   789220.0    290594   2478830     612468.5  void cutlass__5x_cudnn::Kernel<cutlass__5x_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass__5x…\n",
      "      0.2         15687206        102   153796.1   101184.5     14176   1103045     164612.4  _5x_cudnn_ampere_fp16_scudnn_fp16_128x64_relu_interior_nn_v1                                        \n",
      "      0.2         15316208         10  1531620.8  1378935.0    656388   2380013     646204.7  _5x_cudnn_ampere_fp16_scudnn_fp16_128x128_stridedB_small_nn_v1                                      \n",
      "      0.2         15252690         16   953293.1   760900.0    367906   2476046     618800.5  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816wgrad_optimized_f16_64x64_64x5_nhwc_align…\n",
      "      0.1         14253836        425    33538.4    25216.0     23328     67072      15503.6  ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nt                                          \n",
      "      0.1         13770941        325    42372.1     2560.0      2367    552483     137789.9  void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<c10::Half>, at::deta…\n",
      "      0.1         13714895         38   360918.3   256833.5     18464   2159819     456530.1  sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x16_sta…\n",
      "      0.1         13386764         24   557781.8   512547.0    244769    972166     209380.6  sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_war…\n",
      "      0.1         13268999         18   737166.6   698868.0    355937   1258439     288962.8  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x128_32x6_nhwc_alig…\n",
      "      0.1         13172653         38   346648.8   246833.0     17280   2073802     438464.4  sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x16_sta…\n",
      "      0.1         13171304         80   164641.3   108944.5     22881    666499     150501.0  sm86_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage4_war…\n",
      "      0.1         12934658         30   431155.3   418657.5    196705    825445     185770.8  sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_warpsize2x2…\n",
      "      0.1         12424292        104   119464.3    89920.5     24288    462499      94419.9  sm86_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage3_warpsize2x2…\n",
      "      0.1         12314280         64   192410.6   125584.5     27840   1387239     241735.2  _5x_cudnn_ampere_fp16_scudnn_fp16_128x32_relu_small_nn_v1                                           \n",
      "      0.1         11786440         64   184163.1   122673.0     25792   1307495     228701.0  _5x_cudnn_ampere_fp16_scudnn_fp16_128x32_relu_interior_nn_v1                                        \n",
      "      0.1         10654073         16   665879.6   665172.0    238050   1003782     226801.7  sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warp…\n",
      "      0.1         10521814         72   146136.3    84960.5     14976   1199782     201049.5  _5x_cudnn_ampere_fp16_scudnn_fp16_128x64_relu_small_nn_v1                                           \n",
      "      0.1         10444100         98   106572.4    98352.5     29152    653539     101140.5  _5x_cudnn_ampere_fp16_s16816cudnn_fp16_128x128_ldg8_relu_f2f_exp_large_nhwc_tn_v1                   \n",
      "      0.1         10393303         14   742378.8   700516.0    572771   1010822     157691.5  sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warp…\n",
      "      0.1         10274594         40   256864.9   206657.5     76865    545379     151230.5  _5x_cudnn_ampere_fp16_scudnn_fp16_128x128_relu_interior_nn_v1                                       \n",
      "      0.1         10247415        124    82640.4    49904.5      5888    680708     117604.3  sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x64_sta…\n",
      "      0.1         10057022        163    61699.5    37568.0      4640    718820      87732.1  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_optimized_f16_64x64_32x10_nhwc_alig…\n",
      "      0.1          9964948         10   996494.8   843396.0    369826   2492813     821954.6  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816wgrad_optimized_f16_64x128_64x3_nhwc_alig…\n",
      "      0.1          9939060         26   382271.5   439346.0    219328    500930     110316.5  void cutlass__5x_cudnn::Kernel<cutlass__5x_cudnn::conv::kernel::ImplicitGemmConvolution<cutlass__5x…\n",
      "      0.1          9653778         20   482688.9   417922.5    230369    863333     211722.2  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warp…\n",
      "      0.1          9183696         12   765308.0   749251.5    521987   1100038     192302.8  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_analytic_f16_128x128_32x3_nhwc_alig…\n",
      "      0.1          8710798         18   483933.2   438642.0    201313    839012     207644.4  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_128x128_32x4_nhwc_uni…\n",
      "      0.1          8708718         16   544294.9   518563.0    229409    877893     235724.1  sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_war…\n",
      "      0.1          8388363        325    25810.3    25824.0     24577     26880        438.6  ampere_fp16_s1688gemm_fp16_256x64_ldg8_f2f_stages_32x1_tn                                           \n",
      "      0.1          8154093         12   679507.8   680147.5    456259    907909     222727.8  ampere_s1688gemm_fp16_256x64_ldg8_stages_32x1_tn                                                    \n",
      "      0.1          7939401          8   992425.1   977076.5    785828   1227463     170563.9  sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x32x32_stage4_warp…\n",
      "      0.1          7074178         10   707417.8   666259.5    573890    987109     156455.2  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_analytic_f16_64x256_32x4_nhwc_align…\n",
      "      0.1          6246497         10   624649.7   572787.0    480354    911109     158891.9  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x256_32x4_nhwc_alig…\n",
      "      0.1          6237954          2  3118977.0  3118977.0   3087025   3150929      45187.0  sm80_xmma_wgrad_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x16x64_stag…\n",
      "      0.1          5862974         14   418783.9   341665.5    232898    677251     174873.2  sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warp…\n",
      "      0.1          5754623         14   411044.5   341362.0    255969    629571     136812.3  sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage5_warpsize2x2x…\n",
      "      0.1          5540380          8   692547.5   749555.5    306241    970501     263785.7  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize32x128x32_stage4_warp…\n",
      "      0.1          5323067         12   443588.9   325122.0    203969    864996     270937.9  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_war…\n",
      "      0.1          4896228        149    32860.6    31232.0      4992     59200      15706.1  void cutlass::Kernel2<cutlass_80_tensorop_f16_s16816gemm_relu_f16_64x64_32x6_nn_align8>(T1::Params) \n",
      "      0.0          4805748        134    35863.8    22656.5      5120    358881      45445.9  sm86_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage3_warps…\n",
      "      0.0          4665112         10   466511.2   437954.5    216865    729507     206941.4  sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x…\n",
      "      0.0          4596633         61    75354.6    38912.0      3712    148513      59283.2  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<c10::Half, at::native::MeanOp…\n",
      "      0.0          4579129          6   763188.2   675652.0    596739   1024390     198798.3  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_256x64_32x4_nhwc_alig…\n",
      "      0.0          4503644         72    62550.6    57024.0     23361    104897      25969.8  void cutlass::Kernel2<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_128x2_nn_align8>(T1::Param…\n",
      "      0.0          4383701         75    58449.3    58432.0     56864     60224        731.2  ampere_fp16_s1688gemm_fp16_256x64_sliced1x2_ldg8_relu_f2f_tn                                        \n",
      "      0.0          4265174         10   426517.4   316674.0    229505    700292     217004.0  sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_war…\n",
      "      0.0          4217786         10   421778.6   421955.0    294305    580803     105150.4  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x128_64x3_nhwc_unit…\n",
      "      0.0          4209914         56    75177.0    48112.5     12800    226017      65259.8  sm86_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x32_stage6_warpsize2x2x…\n",
      "      0.0          4058162          8   507270.3   462562.0    435234    669987     101778.7  sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warpsize4x1x…\n",
      "      0.0          3897365          8   487170.6   413138.0    368642    753156     165811.0  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_64x64_64x5_nhwc_unity…\n",
      "      0.0          3555636          4   888909.0   887445.0    707172   1073574     209426.1  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_128x64_64x3_nhwc_alig…\n",
      "      0.0          3497874          8   437234.3   440418.0    229153    638852     155156.1  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816dgrad_optimized_f16_256x64_32x4_nhwc_unit…\n",
      "      0.0          3410064          8   426258.0   477682.0    234529    515139     119422.5  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x64_stage3_war…\n",
      "      0.0          3120643        425     7342.7     7040.0      5408     10080       1082.5  void at::native::reduce_kernel<(int)128, (int)4, at::native::ReduceOp<c10::Half, at::native::func_w…\n",
      "      0.0          2926095          6   487682.5    37360.0     36353   1411463     698338.0  sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x16x64_sta…\n",
      "      0.0          2924817        600     4874.7     4896.0      1664      8000       1531.9  void at::native::elementwise_kernel<(int)128, (int)4, void at::native::gpu_kernel_impl_nocast<at::n…\n",
      "      0.0          2661999          6   443666.5   440514.5    429091    470530      15609.0  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_war…\n",
      "      0.0          2610231       1325     1970.0     1952.0      1792      3168        102.8  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctorOnSelf_add<long>, at:…\n",
      "      0.0          2381773          2  1190886.5  1190886.5   1190214   1191559        951.1  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize32x256x32_stage3_warp…\n",
      "      0.0          2305880        410     5624.1     5824.0      3264      7937        933.3  void cublasLt::splitKreduce_kernel<(int)32, (int)16, int, __half, __half, float, __half, (bool)1, (…\n",
      "      0.0          2207178        360     6131.1     6304.0      4992      6944        510.1  void at::native::<unnamed>::vectorized_layer_norm_kernel<float, float>(int, T2, const T1 *, const T…\n",
      "      0.0          2156426         18   119801.4   109232.0     62401    184545      38438.8  sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x128x32_stage3_war…\n",
      "      0.0          2155816        454     4748.5     4769.0      1568      5601        334.1  void cublasLt::splitKreduce_kernel<(int)32, (int)16, int, __half, __half, float, __half, (bool)1, (…\n",
      "      0.0          2079855         48    43330.3    13760.0      4545    663844     130882.6  sm86_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warpsize2x2x1…\n",
      "      0.0          2031786          4   507946.5   507634.5    296641    719876     243663.3  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warps…\n",
      "      0.0          1990250         28    71080.4    68208.0     12160    128097      47402.0  sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x64_sta…\n",
      "      0.0          1961897        522     3758.4     1055.0       960     72128      10010.7  void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<float>, at::detail::…\n",
      "      0.0          1959595          6   326599.2    45168.5     43168    894373     437433.2  sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x16_sta…\n",
      "      0.0          1937353          4   484338.3   484178.0      4512    964485     553280.5  sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x64_sta…\n",
      "      0.0          1927753         10   192775.3    34912.0      4256    881956     363192.8  sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x16x32_sta…\n",
      "      0.0          1922160         25    76886.4    76992.0     74977     78048        765.3  void at::native::elementwise_kernel<(int)128, (int)4, void at::native::gpu_kernel_impl_nocast<at::n…\n",
      "      0.0          1817738          6   302956.3    34144.5     33184    842148     417148.8  sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x16x64_sta…\n",
      "      0.0          1742025          8   217753.1    27824.5      4192    810884     366050.8  sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x64_sta…\n",
      "      0.0          1592333        200     7961.7     8240.0      5760      9344       1144.2  void at::native::<unnamed>::layer_norm_grad_input_kernel_vectorized<float, float>(const T1 *, const…\n",
      "      0.0          1543562         32    48236.3    48256.0     47713     48960        259.6  ampere_fp16_s16816gemm_fp16_256x128_ldg8_f2f_stages_32x3_tn                                         \n",
      "      0.0          1487148        315     4721.1     4576.0      3552      6304        734.8  void at::native::unrolled_elementwise_kernel<at::native::CUDAFunctor_add<float>, at::detail::Array<…\n",
      "      0.0          1431658         25    57266.3    57313.0     55777     58432        684.1  ampere_fp16_s1688gemm_fp16_256x64_sliced1x2_ldg8_f2f_tn                                             \n",
      "      0.0          1348959        100    13489.6    13472.5     13024     14592        211.3  void pytorch_flash::flash_bwd_dq_dk_dv_loop_seqk_parallel_kernel<pytorch_flash::Flash_bwd_kernel_tr…\n",
      "      0.0          1274852        315     4047.1     5440.0      1120      6080       2040.6  void at::native::vectorized_elementwise_kernel<(int)4, at::native::CUDAFunctor_add<float>, at::deta…\n",
      "      0.0          1267820        699     1813.8     1823.0      1407      2335        103.6  void cask__5x_cudnn::computeOffsetsKernel<(bool)0, (bool)0>(cask__5x_cudnn::ComputeOffsetsParams)   \n",
      "      0.0          1254093         28    44789.0    44832.0     43872     45440        411.2  ampere_fp16_s16816gemm_fp16_128x64_ldg8_relu_f2f_stages_64x4_tn                                     \n",
      "      0.0          1187622          2   593811.0   593811.0    592803    594819       1425.5  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816wgrad_optimized_f16_128x64_32x6_nhwc_alig…\n",
      "      0.0          1163395         65    17898.4    17889.0     17664     18209        139.7  ampere_fp16_s16816gemm_fp16_128x128_ldg8_f2f_stages_64x3_tn                                         \n",
      "      0.0          1152040        148     7784.1     7872.0      6656      8288        358.9  void pytorch_flash::flash_fwd_kernel<pytorch_flash::Flash_fwd_kernel_traits<(int)256, (int)64, (int…\n",
      "      0.0          1106510        200     5532.6     5792.0      3969      6240        590.6  void at::native::<unnamed>::GammaBetaBackwardCUDAKernel<float, float>(long, long, const T1 *, const…\n",
      "      0.0          1023526          2   511763.0   511763.0    511107    512419        927.7  sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x64x64_stage3_warpsize2x2x…\n",
      "      0.0          1016870         87    11688.2    10336.0      2656     87329      13398.7  void cutlass__5x_cudnn::Kernel<cutlass__5x_cudnn::reduction::kernel::ReduceSplitK<cutlass__5x_cudnn…\n",
      "      0.0          1015016         65    15615.6    15616.0     15007     16289        312.7  ampere_s16816gemm_fp16_128x64_ldg8_stages_64x3_tn                                                   \n",
      "      0.0           990534        126     7861.4     6240.0      2945     39136       4690.7  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x128x32_stage4_war…\n",
      "      0.0           745252         22    33875.1    30288.5      8288     56385      17100.9  sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x…\n",
      "      0.0           690659         40    17266.5    14623.5      5536     32064       8853.8  sm86_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage4_warps…\n",
      "      0.0           674278         42    16054.2    12704.5      5248     32768       8380.9  sm80_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warps…\n",
      "      0.0           671975         47    14297.3    11136.0      6304     31360       7130.1  sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1…\n",
      "      0.0           640446         39    16421.7    16416.0     15968     16768        179.6  ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_relu_f2f_stages_64x6_tn                            \n",
      "      0.0           611364         28    21834.4    23952.0      8064     37088       7235.2  sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage4_warpsize2x2x1…\n",
      "      0.0           594980          2   297490.0   297490.0    296898    298082        837.2  void foldedNhwcToNchwKernel<__half, __half, float, (bool)1, (cudnnKernelDataType_t)0>(int, int, int…\n",
      "      0.0           592168        100     5921.7     5872.0      5440      7200        305.0  void pytorch_flash::flash_bwd_dot_do_o_kernel<(bool)1, pytorch_flash::Flash_bwd_kernel_traits<(int)…\n",
      "      0.0           584105        188     3106.9     2688.0      1824      5025       1138.6  void xmma__5x_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_3<xmma__5x_cudnn::im…\n",
      "      0.0           560285        188     2980.2     2976.0      2624      3264        162.1  void xmma__5x_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_1<xmma__5x_cudnn::im…\n",
      "      0.0           547235          2   273617.5   273617.5    273250    273985        519.7  sm80_xmma_dgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x32_stage4_warp…\n",
      "      0.0           546723          2   273361.5   273361.5    272033    274690       1878.8  sm80_xmma_dgrad_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x…\n",
      "      0.0           535783         32    16743.2    15408.5      6304     42752       9286.2  sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x32x32_stage4_warpsize4x1x…\n",
      "      0.0           535620         12    44635.0    44513.0     44096     45408        393.6  ampere_fp16_s16816gemm_fp16_128x64_ldg8_relu_f2f_stages_32x6_tn                                     \n",
      "      0.0           531300         34    15626.5    13472.5      7840     41120       7696.8  sm86_xmma_fprop_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x64x64_stage6_warps…\n",
      "      0.0           463039        100     4630.4     4720.0      3840      5023        287.3  void pytorch_flash::flash_bwd_convert_dq_kernel<pytorch_flash::Flash_bwd_kernel_traits<(int)256, (i…\n",
      "      0.0           461985        188     2457.4     2464.0      1792      2657         98.3  void xmma__5x_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_0<xmma__5x_cudnn::im…\n",
      "      0.0           448930         26    17266.5    17312.0     16192     18112        549.3  ampere_fp16_s16816gemm_fp16_64x64_sliced1x2_ldg8_relu_f2f_stages_64x5_tn                            \n",
      "      0.0           423461         26    16287.0    16400.5     15489     16928        405.5  void cutlass::Kernel2<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_32x32_128x2_tn_align8>(T1::Param…\n",
      "      0.0           420163         75     5602.2     6624.0      3200      7136       1615.4  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::n…\n",
      "      0.0           416192        120     3468.3     3360.0      1600      6176       1435.5  void at::native::elementwise_kernel<(int)128, (int)2, void at::native::gpu_kernel_impl_nocast<at::n…\n",
      "      0.0           386143         75     5148.6     4448.0      3872      7232       1342.0  void at::native::elementwise_kernel<(int)128, (int)4, void at::native::gpu_kernel_impl_nocast<at::n…\n",
      "      0.0           359817        188     1913.9     1824.0      1632      2400        204.5  void xmma__5x_cudnn::implicit_gemm::strided_dgrad_indexed::kernel_helper_stage_2<xmma__5x_cudnn::im…\n",
      "      0.0           341827         32    10682.1    10656.0     10112     11233        215.9  void pytorch_flash::flash_fwd_splitkv_kernel<pytorch_flash::Flash_fwd_kernel_traits<(int)256, (int)…\n",
      "      0.0           333730          8    41716.3    41568.0     41216     42400        474.7  void cutlass::Kernel2<cutlass_80_wmma_tensorop_f16_s161616gemm_f16_16x16_128x2_tn_align8>(T1::Param…\n",
      "      0.0           322913         17    18994.9    13472.0     10399     34752       9340.7  sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x32x64_stage5_warpsize2x2x1…\n",
      "      0.0           301378         95     3172.4     3232.0      2752      3552        242.1  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<c10::Half, at::native::func_w…\n",
      "      0.0           290913         45     6464.7     6720.0      5247      7680        614.1  void at::native::reduce_kernel<(int)128, (int)4, at::native::ReduceOp<float, at::native::func_wrapp…\n",
      "      0.0           282881         70     4041.2     3136.0      2687      6240       1465.7  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::func_wrapp…\n",
      "      0.0           279554         14    19968.1    21791.5      9121     27585       6430.2  sm80_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x32x32_stage4_warpsize4x1x…\n",
      "      0.0           276996         90     3077.7     3232.0      2208      3648        455.2  void cublasLt::splitKreduce_kernel<(int)32, (int)16, int, float, __half, float, __half, (bool)1, (b…\n",
      "      0.0           248603         90     2762.3     2752.0      2336      3104        213.8  void at::native::reduce_kernel<(int)512, (int)1, at::native::ReduceOp<float, at::native::MeanOps<fl…\n",
      "      0.0           219588         13    16891.4    16928.0     16544     17216        196.5  ampere_fp16_s16816gemm_fp16_64x64_ldg8_relu_f2f_stages_64x6_tn                                      \n",
      "      0.0           219359         13    16873.8    16896.0     16448     17152        207.0  ampere_fp16_s16816gemm_fp16_64x64_ldg8_relu_f2f_stages_64x5_tn                                      \n",
      "      0.0           215293         13    16561.0    16512.0     15871     17120        315.6  ampere_fp16_s16816gemm_fp16_128x64_ldg8_f2f_stages_32x6_tn                                          \n",
      "      0.0           199744         12    16645.3     5568.0      3776     41504      17519.6  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize128x256x32_stage3_war…\n",
      "      0.0           182981         16    11436.3     4528.5      3489     56513      17478.9  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize256x64x32_stage3_warp…\n",
      "      0.0           182017         29     6276.4     5824.0      5568      8320        871.7  sm80_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize32x32x64_stage6_warpsize2x2x1_tensor16x8x16_kernel    \n",
      "      0.0           157922          4    39480.5    39376.5      8608     70561      35483.6  void nchwToFoldedNhwcKernel<__half, __half, float, (bool)1, (cudnnKernelDataType_t)0>(int, int, int…\n",
      "      0.0           152540        120     1271.2     1280.0      1152      1504         71.7  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BinaryFunctor<float, float, floa…\n",
      "      0.0           124639         25     4985.6     4992.0      4384      5152        146.0  void cutlass::Kernel2<cutlass_75_wmma_tensorop_s161616gemm_f16_16x16_128x1_tn_align1>(T1::Params)   \n",
      "      0.0           114561         19     6029.5     5984.0      5536      6528        339.1  ampere_fp16_s16816gemm_fp16_128x64_ldg8_f2f_stages_32x6_nn                                          \n",
      "      0.0           113537         25     4541.5     4544.0      4064      4768        132.9  std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, __half, __half, __half, float, (b…\n",
      "      0.0           102467         25     4098.7     4097.0      3840      4224         82.0  ampere_fp16_sgemm_fp16_128x32_nn                                                                    \n",
      "      0.0            99295         14     7092.5     5264.0      3424     16128       4210.2  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize64x128x32_stage5_warp…\n",
      "      0.0            97855         45     2174.6     2240.0      1856      2336        134.7  void at::native::vectorized_elementwise_kernel<(int)4, at::native::tanh_kernel_cuda(at::TensorItera…\n",
      "      0.0            96991         28     3464.0     3504.0      3103      3617        155.8  std::enable_if<!T7, void>::type internal::gemvx::kernel<int, int, __half, __half, __half, float, (b…\n",
      "      0.0            94015         45     2089.2     2048.0      1792      2304        101.3  void at::native::unrolled_elementwise_kernel<at::native::BinaryFunctor<float, float, bool, at::nati…\n",
      "      0.0            93091         32     2909.1     2928.5      2720      3040         99.1  void pytorch_flash::flash_fwd_splitkv_combine_kernel<pytorch_flash::Flash_fwd_kernel_traits<(int)25…\n",
      "      0.0            88865         16     5554.1     5504.0      5376      5824        135.1  ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nn                                          \n",
      "      0.0            86752         25     3470.1     3456.0      3328      3840        108.1  void at::native::vectorized_elementwise_kernel<(int)4, at::native::tanh_backward_kernel_cuda(at::Te…\n",
      "      0.0            81731         16     5108.2     5104.0      4928      5312        122.5  ampere_fp16_s16816gemm_fp16_128x64_ldg8_f2f_stages_64x3_nn                                          \n",
      "      0.0            76896         25     3075.8     3072.0      2784      3201         83.9  void cutlass::Kernel2<cutlass_75_wmma_tensorop_f16_s161616gemm_f16_32x32_64x1_nt_align1>(T1::Params)\n",
      "      0.0            76704         45     1704.5     1888.0      1375      2080        266.7  void at::native::vectorized_elementwise_kernel<(int)4, at::native::sigmoid_kernel_cuda(at::TensorIt…\n",
      "      0.0            76132         25     3045.3     3040.0      2784      3169         78.9  void cutlass::Kernel2<cutlass_75_wmma_tensorop_f16_s161616gemm_f16_32x32_64x1_nn_align1>(T1::Params)\n",
      "      0.0            75585         37     2042.8     1696.0      1568      3103        534.7  void dot_kernel<float, (int)128, (int)0, cublasDotParams<cublasGemvTensorStridedBatched<const __hal…\n",
      "      0.0            69503         45     1544.5     1600.0      1376      1760        112.3  void at::native::vectorized_elementwise_kernel<(int)4, at::native::launch_log_sigmoid_forward_kerne…\n",
      "      0.0            64675         37     1748.0     1760.0      1632      1857         65.8  void reduce_1Block_kernel<float, (int)128, (int)7, cublasGemvTensorStridedBatched<float>, cublasGem…\n",
      "      0.0            59871         45     1330.5     1375.0      1184      1504         88.8  void at::native::vectorized_elementwise_kernel<(int)4, at::native::round_kernel_cuda(at::TensorIter…\n",
      "      0.0            56416         25     2256.6     2272.0      2048      2400         67.9  at::native::amp_update_scale_cuda_kernel(float *, int *, const float *, double, double, int)        \n",
      "      0.0            54597         32     1706.2     1728.5      1536      1793         90.6  void <unnamed>::softmax_warp_forward<c10::Half, float, float, (int)6, (bool)0, (bool)0>(T2 *, const…\n",
      "      0.0            54111         45     1202.5     1184.0      1183      1280         25.9  void at::native::vectorized_elementwise_kernel<(int)4, at::native::round_kernel_cuda(at::TensorIter…\n",
      "      0.0            48862         25     1954.5     1952.0      1887      2112         54.6  void at::native::unrolled_elementwise_kernel<at::native::direct_copy_kernel_cuda(at::TensorIterator…\n",
      "      0.0            41473         25     1658.9     1664.0      1568      1728         37.7  void at::native::vectorized_elementwise_kernel<(int)4, at::native::reciprocal_kernel_cuda(at::Tenso…\n",
      "      0.0            40928         13     3148.3     3104.0      2880      3456        175.7  void at::native::<unnamed>::CatArrayBatchedCopy_contig<at::native::<unnamed>::OpaqueType<(unsigned …\n",
      "      0.0            36352         25     1454.1     1441.0      1375      1504         30.8  void at::native::vectorized_elementwise_kernel<(int)4, at::native::sigmoid_kernel_cuda(at::TensorIt…\n",
      "      0.0            35106         25     1404.2     1408.0      1344      1440         25.0  void <unnamed>::softmax_warp_backward<float, c10::Half, float, (int)6, (bool)0, (bool)0>(T2 *, cons…\n",
      "      0.0            31841         25     1273.6     1280.0      1216      1312         26.0  void at::native::vectorized_elementwise_kernel<(int)4, at::native::BUnaryFunctor<float, float, floa…\n",
      "      0.0            19710         10     1971.0     1871.5      1696      2335        234.6  void cask__5x_cudnn::computeOffsetsKernel<(bool)1, (bool)0>(cask__5x_cudnn::ComputeOffsetsParams)   \n",
      "      0.0            17536          4     4384.0     4256.0      3712      5312        728.8  void cutlass::Kernel2<cutlass_80_tensorop_f16_s16816gemm_relu_f16_64x64_32x10_nn_align8>(T1::Params)\n",
      "      0.0            16322         10     1632.2     1648.0      1504      1729         65.9  cask__5x_cudnn::computeBOffsetsKernel(cask__5x_cudnn::ComputeBOffsetsParams)                        \n",
      "      0.0            14849          2     7424.5     7424.5      7200      7649        317.5  sm80_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize96x128x32_stage4_warpsize2x2x1_tensor16x8x16_kernel   \n",
      "      0.0            13664          8     1708.0     1696.0      1696      1760         23.8  void <unnamed>::softmax_warp_forward<c10::Half, float, float, (int)7, (bool)0, (bool)0>(T2 *, const…\n",
      "      0.0            13152          2     6576.0     6576.0      6208      6944        520.4  sm80_xmma_wgrad_implicit_gemm_indexed_f16f16_f16f32_f32_nhwckrsc_nhwc_tilesize32x256x32_stage3_warp…\n",
      "      0.0            12224          2     6112.0     6112.0      5664      6560        633.6  void cutlass::Kernel2<cutlass_80_tensorop_f16_s16816gemm_relu_f16_256x64_32x4_nn_align8>(T1::Params)\n",
      "      0.0            12096          2     6048.0     6048.0      5728      6368        452.5  void cutlass__5x_cudnn::Kernel<cutlass_tensorop_f16_s16816fprop_analytic_f16_128x64_32x6_nhwc_align…\n",
      "      0.0             9024          2     4512.0     4512.0      4320      4704        271.5  void cutlass::Kernel2<cutlass_80_tensorop_f16_s16816gemm_relu_f16_128x64_32x6_nn_align8>(T1::Params)\n",
      "      0.0             5984          1     5984.0     5984.0      5984      5984          0.0  void at::native::<unnamed>::CatArrayBatchedCopy_contig<at::native::<unnamed>::OpaqueType<(unsigned …\n",
      "      0.0             4544          3     1514.7     1504.0      1472      1568         48.9  void <unnamed>::softmax_warp_forward<c10::Half, float, float, (int)5, (bool)0, (bool)0>(T2 *, const…\n",
      "      0.0             2912          2     1456.0     1456.0      1440      1472         22.6  void <unnamed>::softmax_warp_forward<c10::Half, float, float, (int)4, (bool)0, (bool)0>(T2 *, const…\n",
      "      0.0             1856          1     1856.0     1856.0      1856      1856          0.0  void at::native::<unnamed>::CatArrayBatchedCopy_aligned16_contig<at::native::<unnamed>::OpaqueType<…\n",
      "      0.0             1023          1     1023.0     1023.0      1023      1023          0.0  void at::native::vectorized_elementwise_kernel<(int)4, at::native::FillFunctor<int>, at::detail::Ar…\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)   Med (ns)  Min (ns)  Max (ns)  StdDev (ns)            Operation           \n",
      " --------  ---------------  -----  ---------  --------  --------  --------  -----------  ------------------------------\n",
      "     95.9        478145675    462  1034947.3     544.0       415  14197512    2924796.8  [CUDA memcpy Host-to-Device]  \n",
      "      3.9         19506311   2486     7846.5    1184.0       415    274466      29454.7  [CUDA memset]                 \n",
      "      0.2           946470    377     2510.5    2689.0      1120      3392        693.4  [CUDA memcpy Device-to-Device]\n",
      "      0.0           158717    117     1356.6    1345.0       959      2304        344.9  [CUDA memcpy Device-to-Host]  \n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)            Operation           \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ------------------------------\n",
      "  12497.501   2486     5.027     0.001     0.000   184.549       20.758  [CUDA memset]                 \n",
      "   4827.642    462    10.449     0.002     0.000   138.412       32.538  [CUDA memcpy Host-to-Device]  \n",
      "    216.269    377     0.574     0.721     0.000     0.721        0.291  [CUDA memcpy Device-to-Device]\n",
      "      0.001    117     0.000     0.000     0.000     0.000        0.000  [CUDA memcpy Device-to-Host]  \n",
      "\n",
      "Generated:\n",
      "    /datasets/AZ_Workshop/profile_output.nsys-rep\n",
      "    /datasets/AZ_Workshop/profile_output.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile -t nvtx,cuda,osrt --cuda-memory-usage=true --force-overwrite=true --stats=true --output=profile_output python  -u prof_mil_script.py \\\n",
    "    --data_root=/Panda/train_images \\\n",
    "    --amp \\\n",
    "    --mil_mode=att_trans \\\n",
    "    --batch_size=4 \\\n",
    "    --epochs=1 \n",
    "\n",
    "#!python -u prof_mil_script.py \\\n",
    "#    --data_root=/Panda/train_images \\\n",
    "#    --amp \\\n",
    "#    --mil_mode=att_trans \\\n",
    "#    --batch_size=4 \\\n",
    "#    --epochs=1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d953949-6825-4a92-9957-1cc6d22e47be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
