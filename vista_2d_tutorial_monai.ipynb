{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License. \n",
    "\n",
    "# VISTA-2D Cell Segmentation with Segment Anything Model (SAM) & MONAI\n",
    "\n",
    "![image](../figures/vista_2d_overview.png)\n",
    "\n",
    "The tutorial demonstrates how to train a cell segmentation model using the [MONAI](https://monai.io/) framework and the [Segment Anything Model (SAM)](https://github.com/facebookresearch/segment-anything) on the [Cellpose dataset](https://www.cellpose.org/).\n",
    "In Summary the following steps are performed:\n",
    "- Initialization of the CellSamWrapper model with pre-trained SAM weights\n",
    "- Creation of data lists for training, validation, and testing\n",
    "- Definition of data transforms for training and validation\n",
    "- Setup of datasets and dataloaders with MONAI\n",
    "- Implementation of the training loop, including:\n",
    "    - Loss function (CellLoss)\n",
    "    - Accuracy function (CellAcc)\n",
    "    - Optimizer (SGD)\n",
    "- Mixed precision training with GradScaler\n",
    "- Sliding window inference via MONAI\n",
    "- Visualization of training loss, validation loss, and validation accuracy\n",
    "- Inference on a single validation image\n",
    "- Visualization of input image, ground truth, and model prediction\n",
    "\n",
    "The notebook demonstrates a complete pipeline for training and evaluating a cell segmentation model using the MONAI framework and the Segment Anything Model (SAM) on the Cellpose dataset. Please note we only use a small subset of the cellpose dataset for this tutorial.\n",
    "\n",
    "For additional information about VISTA-2D please also refer the [MONAI bundle](https://github.com/Project-MONAI/VISTA/tree/main/vista2d) and the [technical blog post](https://developer.nvidia.com/blog/advancing-cell-segmentation-and-morphology-analysis-with-nvidia-ai-foundation-model-vista-2d/).\n",
    "\n",
    "For dependency-related issues, you can also check this note:\n",
    "https://github.com/Project-MONAI/model-zoo/tree/dev/models/vista2d#dependencies.\n",
    "If you're using MONAI's container, you can disregard the 'opencv-python-headless not installed' error, as it is already included within the container."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 23 15:45:36 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.6     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              63W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cellpose 3.1.1.1 requires opencv-python-headless, which is not installed.\n",
      "cellpose 3.1.1.1 requires roifile, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cellpose 3.1.1.1 requires opencv-python-headless, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!pip install -q fastremap\n",
    "!pip install -q --no-deps cellpose\n",
    "!pip install -q natsort\n",
    "!pip install -q roifile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.4.0\n",
      "Numpy version: 1.24.4\n",
      "Pytorch version: 2.5.0a0+872d972e41.nv24.08.02\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 46a5272196a6c2590ca2589029eed8e4d56ff008\n",
      "MONAI __file__: /usr/local/lib/python3.10/dist-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.11\n",
      "ITK version: 5.4.0\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.23.2\n",
      "scipy version: 1.13.1\n",
      "Pillow version: 10.4.0\n",
      "Tensorboard version: 2.17.0\n",
      "gdown version: 5.2.0\n",
      "TorchVision version: 0.20.0a0\n",
      "tqdm version: 4.66.4\n",
      "lmdb version: 1.5.1\n",
      "psutil version: 5.9.8\n",
      "pandas version: 2.2.2\n",
      "einops version: 0.7.0\n",
      "transformers version: 4.40.2\n",
      "mlflow version: 2.16.0\n",
      "pynrrd version: 1.0.0\n",
      "clearml version: 1.16.3\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "import shutil\n",
    "import tempfile\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import tifffile\n",
    "import random\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import monai.transforms as mt\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.apps import download_url\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.inferers import SlidingWindowInfererAdapt\n",
    "from monai.metrics import CumulativeAverage\n",
    "from monai.transforms import Compose, LoadImage, EnsureChannelFirst, ScaleIntensity, LoadImaged, MapTransform\n",
    "from monai.utils import ImageMetaKey\n",
    "from monai.networks.nets.cell_sam_wrapper import CellSamWrapper\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from scripts.components import CellAcc, CellLoss, LabelsToFlows, LoadTiffd, LogitsToLabels\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup data directory\n",
    "\n",
    "Set the root_dir to the folder/volume that was mounted, containing the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - change this to your (mounted) directory\n",
    "root_dir = \"/datasets/AZ_Workshop\"\n",
    "\n",
    "if not os.path.exists(os.path.join(root_dir,\"data\")):\n",
    "    os.mkdir(os.path.join(root_dir,\"data\"))\n",
    "\n",
    "if not os.path.exists(os.path.join(root_dir,\"vista2d\")):\n",
    "    os.mkdir(os.path.join(root_dir,\"vista2d\"))\n",
    "\n",
    "zip_file_dir = \"/datasets/AZ_Workshop/data/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset and Pre-trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-23 16:09:47,102 - INFO - Expected md5 is None, skip md5 check for file /datasets/AZ_Workshop/cellpose_toy_datalist.json.\n",
      "2025-02-23 16:09:47,102 - INFO - File exists: /datasets/AZ_Workshop/cellpose_toy_datalist.json, skipped downloading.\n",
      "2025-02-23 16:09:47,103 - INFO - Expected md5 is None, skip md5 check for file /datasets/AZ_Workshop/sam_vit_b_01ec64.pth.\n",
      "2025-02-23 16:09:47,103 - INFO - File exists: /datasets/AZ_Workshop/sam_vit_b_01ec64.pth, skipped downloading.\n"
     ]
    }
   ],
   "source": [
    "data_list_path = os.path.join(root_dir, \"cellpose_toy_datalist.json\")\n",
    "data_list_path_url = \"https://developer.download.nvidia.com/assets/Clara/monai/tutorials/cellpose_toy_datalist.json\"\n",
    "download_url(url=data_list_path_url, filepath=data_list_path)\n",
    "\n",
    "sam_weights_path_url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\"\n",
    "sam_weights_path = os.path.join(root_dir, \"sam_vit_b_01ec64.pth\")\n",
    "download_url(url=sam_weights_path_url, filepath=sam_weights_path)\n",
    "\n",
    "# Should reflect the location of the Cellpose dataset on your local machine\n",
    "vista2d_root = os.path.join(root_dir, \"vista2d\")\n",
    "cellpose_data_root = os.path.join(root_dir, \"data\", \"cellpose\")\n",
    "if not os.path.exists(cellpose_data_root):\n",
    "    os.mkdir(cellpose_data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CellPose data, used in the training example below, can be downloaded from [here](https://drive.google.com/file/d/1OB5u1X7YYTgMIu8p3kXJNkwJT7N4GSYG/view?usp=drive_link) or you can just run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "#execute in the cellpose_data_root folder\n",
    "!gdown 1OB5u1X7YYTgMIu8p3kXJNkwJT7N4GSYG\n",
    "#then:\n",
    "!tar -xvf cellpose.tar\n",
    "\n",
    "cellpose_filename = os.path.join(cellpose_data_root,\"cellpose.tar\")\n",
    "\n",
    "# Open the tar file\n",
    "with tarfile.open(os.path.join(cellpose_filename, \"r\") as tar:\n",
    "    # Extract all contents to a directory\n",
    "    tar.extractall(path=cellpose_data_root)\n",
    "\n",
    "# now remove the .tar file\n",
    "os.remove(cellpose_filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network, load SAM weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM ViT-B weights loaded succesfully ...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CellSamWrapper(checkpoint=sam_weights_path)\n",
    "model.to(device)\n",
    "print(\"SAM ViT-B weights loaded succesfully ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Required Data lists\n",
    "# Append root path to training, validation and testing data list\n",
    "with open(data_list_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "validation_fold = 0\n",
    "training_list = []\n",
    "validation_list = []\n",
    "testing_list = []\n",
    "\n",
    "# Process training data\n",
    "for item in data.get(\"training\", []):\n",
    "    # Append the base path to image and label\n",
    "    item[\"image\"] = os.path.join(cellpose_data_root, item[\"image\"])\n",
    "    item[\"label\"] = os.path.join(cellpose_data_root, item[\"label\"])\n",
    "\n",
    "    training_list.append(item)\n",
    "\n",
    "# Process testing data\n",
    "i = 0\n",
    "for item in data.get(\"testing\", []):\n",
    "    # Append the base path to image and label\n",
    "    item[\"image\"] = os.path.join(cellpose_data_root, item[\"image\"])\n",
    "    item[\"label\"] = os.path.join(cellpose_data_root, item[\"label\"])\n",
    "    if i % 2==0:\n",
    "        testing_list.append(item)\n",
    "    else:\n",
    "        validation_list.append(item)\n",
    "    i+=1\n",
    "        \n",
    "print(\"Appended Data Root to Json file list ...\")\n",
    "print(\"Total Training Data: {}\".format(len(training_list)))\n",
    "print(\"Total Validation Data: {}\".format(len(validation_list)))\n",
    "print(\"Total Testing Data: {}\".format(len(testing_list)))\n",
    "#print(training_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_size = [256, 256]\n",
    "train_transforms = mt.Compose(\n",
    "    [\n",
    "        LoadTiffd(keys=[\"image\", \"label\"]),\n",
    "        mt.EnsureTyped(keys=[\"image\", \"label\"], data_type=\"tensor\", dtype=torch.float),\n",
    "        mt.ScaleIntensityd(keys=\"image\", minv=0, maxv=1, channel_wise=True),\n",
    "        mt.ScaleIntensityRangePercentilesd(\n",
    "            keys=\"image\",\n",
    "            lower=1,\n",
    "            upper=99,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            channel_wise=True,\n",
    "            clip=True,\n",
    "        ),\n",
    "        mt.SpatialPadd(keys=[\"image\", \"label\"], spatial_size=roi_size),\n",
    "        mt.RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=roi_size),\n",
    "        mt.RandAffined(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            prob=0.5,\n",
    "            rotate_range=np.pi,\n",
    "            scale_range=[-0.5, 0.5],\n",
    "            mode=[\"bilinear\", \"nearest\"],\n",
    "            spatial_size=roi_size,\n",
    "            cache_grid=True,\n",
    "            padding_mode=\"border\",\n",
    "        ),\n",
    "        mt.RandAxisFlipd(keys=[\"image\", \"label\"], prob=0.5),\n",
    "        mt.RandGaussianNoised(keys=[\"image\"], prob=0.25, mean=0, std=0.1),\n",
    "        mt.RandAdjustContrastd(keys=[\"image\"], prob=0.25, gamma=(1, 2)),\n",
    "        mt.RandGaussianSmoothd(keys=[\"image\"], prob=0.25, sigma_x=(1, 2)),\n",
    "        mt.RandHistogramShiftd(keys=[\"image\"], prob=0.25, num_control_points=3),\n",
    "        mt.RandGaussianSharpend(keys=[\"image\"], prob=0.25),\n",
    "        LabelsToFlows(keys=\"label\", flow_key=\"flow\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = mt.Compose(\n",
    "    [\n",
    "        LoadTiffd(keys=[\"image\", \"label\"], allow_missing_keys=True),\n",
    "        mt.EnsureTyped(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            data_type=\"tensor\",\n",
    "            dtype=torch.float,\n",
    "            allow_missing_keys=True,\n",
    "        ),\n",
    "        mt.ScaleIntensityRangePercentilesd(\n",
    "            keys=\"image\",\n",
    "            lower=1,\n",
    "            upper=99,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            channel_wise=True,\n",
    "            clip=True,\n",
    "        ),\n",
    "        LabelsToFlows(keys=\"label\", flow_key=\"flow\", allow_missing_keys=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloaders, Losses etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets & Dataloaders for training & validation\n",
    "train_dataset = Dataset(data=training_list, transform=train_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=2)\n",
    "\n",
    "val_dataset = Dataset(data=validation_list, transform=val_transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "# Metrics for Instance Segmentation\n",
    "loss_function = CellLoss()\n",
    "acc_function = CellAcc()\n",
    "\n",
    "# Define Sliding Window Inferer for validation\n",
    "sliding_inferrer = SlidingWindowInfererAdapt(\n",
    "    roi_size=[256, 256], sw_batch_size=1, overlap=0.25, cache_roi_weight_map=True, progress=False\n",
    ")\n",
    "\n",
    "# Define a folder in which to save checkpoints\n",
    "ckpt_path = os.path.join(vista2d_root, \"model_checkpoints\")\n",
    "if os.path.exists(ckpt_path) is False:\n",
    "    os.mkdir(ckpt_path)\n",
    "    \n",
    "# To increase performance, more data would be needed,\n",
    "# epochs can be increased for better results.\n",
    "# Over-fitting is possible, hence please be careful with the number of epochs\n",
    "# Different optimizers can be used as well for varying results.\n",
    "# Please refer to VISTA-2D MONAI Bundle for more information.\n",
    "max_epochs = 10\n",
    "\n",
    "num_epochs_per_validation = 2\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), momentum=0.9, lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "# Amp\n",
    "scaler = GradScaler()\n",
    "amp_dtype = torch.float16\n",
    "\n",
    "best_ckpt_path = os.path.join(ckpt_path, \"model.pt\")\n",
    "intermediate_ckpt_path = os.path.join(ckpt_path, \"model_final.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_metric = -1\n",
    "start_epoch = 0\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "val_epoch_loss_values = []\n",
    "val_epoch_acc_values = []\n",
    "\n",
    "for epoch in range(0, max_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    memory_format = torch.channels_last\n",
    "    run_loss = CumulativeAverage()\n",
    "    avg_loss = avg_acc = 0\n",
    "\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        data = batch_data[\"image\"].as_subclass(torch.Tensor).to(memory_format=memory_format, device=device)\n",
    "        target = batch_data[\"flow\"].as_subclass(torch.Tensor).to(memory_format=memory_format, device=device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Use autocast with float16 for mixed precision training\n",
    "        with autocast(dtype=amp_dtype):\n",
    "            logits = model(data)\n",
    "            loss = loss_function(logits.float(), target)\n",
    "\n",
    "        # Use the scaler for backpropagation and optimization\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        batch_size = data.shape[0]\n",
    "        run_loss.append(loss, count=batch_size)\n",
    "        avg_loss = run_loss.aggregate()\n",
    "\n",
    "        print(f\"Epoch {epoch}/{max_epochs} {idx}/{len(train_loader)} \")\n",
    "        print(f\"loss: {avg_loss:.4f} time {time.time() - start_time:.2f}s \")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss_values.append(avg_loss)\n",
    "\n",
    "    # Validation loop & model checkpoints\n",
    "    if epoch % num_epochs_per_validation == 0:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_run_loss = CumulativeAverage()\n",
    "            run_acc = CumulativeAverage()\n",
    "            for val_idx, val_batch in enumerate(val_loader):\n",
    "                v_data = val_batch[\"image\"].as_subclass(torch.Tensor).to(memory_format=memory_format, device=device)\n",
    "\n",
    "                target = val_batch[\"flow\"].as_subclass(torch.Tensor).to(device=device)\n",
    "\n",
    "                filename = val_batch[\"image\"].meta[ImageMetaKey.FILENAME_OR_OBJ]\n",
    "                batch_size = v_data.shape[0]\n",
    "                loss = acc = None\n",
    "                # Use autocast with float16 for mixed precision validation\n",
    "                with autocast(dtype=amp_dtype):\n",
    "                    logits = sliding_inferrer(inputs=v_data, network=model)\n",
    "                    val_loss = loss_function(logits, target)\n",
    "\n",
    "                val_run_loss.append(val_loss.to(device=device), count=batch_size)\n",
    "                target = None\n",
    "\n",
    "                pred_mask_all = []\n",
    "\n",
    "                for b_ind in range(logits.shape[0]):  # go over batch dim\n",
    "                    pred_mask = LogitsToLabels()(logits=logits[b_ind], filename=filename)\n",
    "                    pred_mask_all.append(pred_mask)\n",
    "\n",
    "                if acc_function is not None:\n",
    "                    label = val_batch[\"label\"].as_subclass(torch.Tensor)\n",
    "\n",
    "                    for b_ind in range(label.shape[0]):\n",
    "                        acc = acc_function(pred_mask_all[b_ind], label[b_ind, 0].long())\n",
    "                        acc = acc.detach().clone() if isinstance(acc, torch.Tensor) else torch.tensor(acc)\n",
    "\n",
    "                        run_acc.append(acc.to(device=device), count=1)\n",
    "                    label = None\n",
    "\n",
    "                avg_loss = val_loss.cpu() if val_loss is not None else 0\n",
    "                avg_acc = acc.cpu().numpy() if acc is not None else 0\n",
    "\n",
    "                print(f\"Val {epoch}/{max_epochs} {val_idx}/{len(val_loader)} \")\n",
    "                print(f\"loss: {avg_loss:.4f} acc {avg_acc}  time {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        val_epoch_loss_values.append(val_run_loss.aggregate())\n",
    "        val_epoch_acc_values.append(run_acc.aggregate())\n",
    "    # Model Saving & Checkpointing\n",
    "    if avg_loss < best_metric:\n",
    "        best_metric = avg_loss\n",
    "        state_dict = model.state_dict()\n",
    "        torch.save({\"state_dict\": state_dict}, best_ckpt_path)\n",
    "        print(f\"Model saved to {best_ckpt_path}\")\n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss plot visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metrics():\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    axs[0].plot(range(0, max_epochs), epoch_loss_values, marker=\"o\")\n",
    "    axs[0].set_title(\"Training Loss\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"Loss Value\")\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    # Plot 2: Validation Loss\n",
    "    axs[1].plot(val_epoch_loss_values, marker=\"o\", color=\"orange\")\n",
    "    axs[1].set_title(\"Validation Loss\")\n",
    "    axs[1].set_xlabel(\"Validation Points\")\n",
    "    axs[1].set_ylabel(\"Val Loss Value\")\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    # Plot 3: Validation Accuracy\n",
    "    axs[2].plot(val_epoch_acc_values, marker=\"x\", color=\"green\")\n",
    "    axs[2].set_title(\"Validation Accuracy\")\n",
    "    axs[2].set_xlabel(\"Validation Points\")\n",
    "    axs[2].set_ylabel(\"Val Accuracy\")\n",
    "    axs[2].grid(True)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "show_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a transform to load and preprocess the image and visualize the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "preprocess = Compose([LoadImage(image_only=True), EnsureChannelFirst(), ScaleIntensity()])\n",
    "\n",
    "n_samples = len(testing_list)\n",
    "random_number = random.randint(0, n_samples)\n",
    "# Load a single validation image (index 2)\n",
    "val_item = testing_list[random_number]\n",
    "image_path = val_item[\"image\"]\n",
    "label_path = val_item[\"label\"]\n",
    "\n",
    "# Load and preprocess the image and label\n",
    "image = preprocess(image_path)\n",
    "label = preprocess(label_path)\n",
    "\n",
    "# Ensure model is in eval mode and on the correct device\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.as_tensor(image).unsqueeze(0).to(device)\n",
    "    with autocast(dtype=amp_dtype):\n",
    "        logits = sliding_inferrer(inputs=input_tensor, network=model)\n",
    "\n",
    "# Convert logits to prediction mask\n",
    "pred_mask = LogitsToLabels()(logits=logits[0])\n",
    "\n",
    "# Move tensors to CPU and convert to numpy for visualization\n",
    "image = image.squeeze().cpu().numpy()\n",
    "label = label.squeeze().cpu().numpy()\n",
    "\n",
    "print(image.shape)\n",
    "print(label.shape)\n",
    "print(pred_mask.shape)\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Handle potential multi-channel images\n",
    "if image.ndim == 3 and image.shape[0] in [1, 3, 4]:\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    if image.shape[2] == 1:\n",
    "        image = image[:, :, 0]\n",
    "\n",
    "axes[0].imshow(image, cmap=\"gray\" if image.ndim == 2 else None)\n",
    "axes[0].set_title(\"Input Image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "if label.ndim == 3 and label.shape[0] == 1:\n",
    "    label = label[0]\n",
    "axes[1].imshow(label, cmap=\"viridis\")\n",
    "axes[1].set_title(\"Ground Truth\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "if pred_mask.ndim == 3 and pred_mask.shape[0] == 1:\n",
    "    pred_mask = pred_mask[0]\n",
    "axes[2].imshow(pred_mask, cmap=\"viridis\")\n",
    "axes[2].set_title(\"Prediction\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "print(f\"Label shape: {label.shape}\")\n",
    "print(f\"Prediction shape: {pred_mask.shape}\")\n",
    "print(f\"Unique values in label: {np.unique(label)}\")\n",
    "print(f\"Unique values in prediction: {np.unique(pred_mask)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now we will try fine-tuning using some more specialised datasets including IHC & H&E\n",
    "\n",
    "The data come from the following public resources:\n",
    "\n",
    "* TNBC 2018\n",
    "* LynSeC\n",
    "* IHC_TMA\n",
    "* MoNuSeg\n",
    "* PANNUKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "from skimage import io\n",
    "from scipy import ndimage\n",
    "import fastremap\n",
    "import skimage\n",
    "\n",
    "Segmentation_Dataset = {}\n",
    "Segmentation_Dataset['Train']=[]\n",
    "Segmentation_Dataset['Test']=[]\n",
    "Segmentation_Dataset['Validation']=[]\n",
    "\n",
    "def extract(zip_dest_path, extract_to):\n",
    "    if not os.path.exists(extract_to):\n",
    "        os.mkdir(extract_to)\n",
    "        \n",
    "    print(f\"Unzipping dataset to {extract_to}...\")\n",
    "    with zipfile.ZipFile(zip_dest_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "\n",
    "    print(\"Unzipping completed.\")\n",
    "\n",
    "def add_item(image, labels, items, dataset, stain=\"H&E\",licence=\"CC BY 4.0\"):\n",
    "    item = {}\n",
    "    item['image']=image\n",
    "    item['label']=labels #fastremap.refit(labels)\n",
    "    item['fold']=0\n",
    "\n",
    "    items.append(item)\n",
    "\n",
    "    return items\n",
    "\n",
    "def separate_touching_objects(lab):\n",
    "    from scipy.ndimage import distance_transform_edt\n",
    "    from skimage.segmentation import watershed\n",
    "    from skimage.measure import label\n",
    "\n",
    "    mask = lab > 0\n",
    "    distance_map = distance_transform_edt(mask) \n",
    "    markers = distance_map > 2\n",
    "    markers = label(markers)\n",
    "    labels = watershed(-distance_map, markers, mask=mask)\n",
    "    labels = labels + lab\n",
    "    \n",
    "    return labels\n",
    "\n",
    "    \n",
    "def show_data(item):\n",
    "    # Plotting\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    temp_dataset = Dataset(data=item, transform=LoadImaged(keys=[\"image\", \"label\"]))\n",
    "    temp_loader = DataLoader(temp_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "    for idx, batch_data in enumerate(temp_loader):\n",
    "        break\n",
    "    \n",
    "    image = np.uint8(batch_data['image'])\n",
    "    label = batch_data['label']\n",
    "    \n",
    "    # Handle potential multi-channel \n",
    "    image = np.squeeze(image)\n",
    "        \n",
    "    if image.ndim == 3 and image.shape[0] in [1, 3, 4]:\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        #if image.shape[2] == 1:\n",
    "        #    image = image[:, :, 0]\n",
    "    if image.ndim == 3 and image.shape[2]>3:\n",
    "        image = image[:,:,:3]\n",
    "\n",
    "    axes[0].imshow(image, cmap=\"gray\" if image.ndim == 2 else None)\n",
    "    axes[0].set_title(\"Sample Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    label = np.squeeze(label)\n",
    "    if label.ndim == 3 and label.shape[0] in [1, 3, 4]:\n",
    "        label = np.transpose(label, (1, 2, 0))\n",
    "\n",
    "    if label.ndim == 3 and label.shape[2]>3:\n",
    "        label = label[:,:,:3]\n",
    "        \n",
    "    axes[1].imshow(label)\n",
    "    axes[1].set_title(\"Ground Truth\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TNBC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnbc_dir = os.path.join(root_dir, \"data\", \"TNBC_NucleiSegmentation\")\n",
    "zip_file_path = os.path.join(zip_file_dir , \"TNBC_NucleiSegmentation.zip\")\n",
    "\n",
    "if not os.path.exists(tnbc_dir):\n",
    "    extract(zip_file_path, tnbc_dir)\n",
    "\n",
    "file_path = os.path.join(tnbc_dir , \"TNBC_and_Brain_dataset\")\n",
    "folders = os.listdir(file_path)\n",
    "\n",
    "items=[]\n",
    "\n",
    "for i,folder in enumerate(tqdm(folders)):\n",
    "    if \"Slide_\" in str(folder):\n",
    "        for file in sorted(os.listdir(os.path.join(file_path,folder))):\n",
    "            file = os.path.join(file_path,folder,file)\n",
    "            if \".DS_Store\" not in str(file):\n",
    "                image = str(file)\n",
    "                im_data = io.imread(image)\n",
    "                if im_data.shape[2]>3:\n",
    "                    im_data = im_data[:,:,:3]\n",
    "                    io.imsave(str(file),im_data)     \n",
    "                masks = str(file).replace(\"Slide_\",\"GT_\")\n",
    "                if os.path.exists(masks):\n",
    "                    mask_data = io.imread(masks)\n",
    "                    # print(np.unique(mask_data))\n",
    "                    mask_data = separate_touching_objects(mask_data)\n",
    "                    mask_data, remapping = fastremap.renumber(mask_data, in_place=True)\n",
    "                    mask_data = fastremap.refit(mask_data)\n",
    "                    os.remove(masks)\n",
    "                    masks = masks.replace(\".png\",\".npy\")\n",
    "                    np.save(masks,mask_data.T)\n",
    "                else:\n",
    "                    masks = masks.replace(\".png\",\".npy\")\n",
    "                    \n",
    "                items = add_item(image, masks, items, \"TNBC_2018\")\n",
    "\n",
    "np.random.seed(42) \n",
    "np.random.shuffle(items)\n",
    "print(\"{} items loaded\".format(len(items)))\n",
    "Segmentation_Dataset['Train']+=items[:int(len(items)*0.8)]\n",
    "Segmentation_Dataset['Validation']+=items[int(len(items)*0.8):int(len(items)*0.9)]\n",
    "Segmentation_Dataset['Test']+=items[int(len(items)*0.9):]\n",
    "\n",
    "show_data(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LyNSec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lynsec_dir= os.path.join(root_dir, \"data\", \"LyNSeC\")\n",
    "zip_file_path = os.path.join(zip_file_dir , \"LyNSeC.zip\")\n",
    "\n",
    "if not os.path.exists(lynsec_dir):\n",
    "    extract(zip_file_path, lynsec_dir)\n",
    "    \n",
    "file_path = lynsec_dir\n",
    "folders = os.listdir(file_path)\n",
    "\n",
    "items = []\n",
    "\n",
    "for i, folder in enumerate(tqdm(folders)):\n",
    "    if folder[:8] == \"lynsec 2\":  # Skip incorrectly annotated folder\n",
    "        #print(\"skip\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    for j, file in enumerate(sorted(os.listdir(os.path.join(file_path,folder)))):\n",
    "        file_ =os.path.join(file_path,folder,file)\n",
    "        if \".DS_Store\" not in str(file_) and \"_lbl\" not in str(file_):\n",
    "            mask_file = str(file_)\n",
    "            image_file = mask_file.replace(\".npy\",\".png\")\n",
    "            if not os.path.exists(image_file):\n",
    "                data = np.load(str(file_))\n",
    "                np.save(mask_file, data[:, :, 3].T)\n",
    "                io.imsave(image_file,data[:, :, :3].astype(np.uint8))\n",
    "            items = add_item(image_file, mask_file, items, \"LyNSeC\")\n",
    "\n",
    "np.random.seed(42) \n",
    "np.random.shuffle(items)\n",
    "print(\"{} items added\".format(len(items)))\n",
    "Segmentation_Dataset['Train'] += items[:int(len(items) * 0.8)]\n",
    "Segmentation_Dataset['Validation'] += items[int(len(items) * 0.8):int(len(items) * 0.9)]\n",
    "Segmentation_Dataset['Test'] += items[int(len(items) * 0.9):]\n",
    "\n",
    "print(\"{} items loaded\".format(len(items)))\n",
    "\n",
    "show_data(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IHC TMA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ihc_tma_dir = os.path.join(root_dir, \"data\", \"IHC_TMA_dataset\")\n",
    "zip_file_path = os.path.join(zip_file_dir , \"IHC_TMA_dataset.zip\")\n",
    "\n",
    "if not os.path.exists(ihc_tma_dir):\n",
    "    extract(zip_file_path, ihc_tma_dir)\n",
    "    \n",
    "file_path = ihc_tma_dir + \"/IHC_TMA_dataset/images\"\n",
    "files = sorted(os.listdir(file_path))\n",
    "\n",
    "items = []\n",
    "\n",
    "for i, file in enumerate(tqdm(files)):\n",
    "    image =os.path.join(file_path,file)\n",
    "    \n",
    "    mask_path = str(os.path.join(file_path,file)).replace(\"images\", \"masks\").replace(\".png\", \".npy\")\n",
    "    masks = np.load(mask_path)\n",
    "    if masks.shape!=(256,256):\n",
    "        n_masks = np.max(masks[0:2], axis=0)\n",
    "    \n",
    "        masks = separate_touching_objects(n_masks)\n",
    "        masks, remapping = fastremap.renumber(masks, in_place=True)\n",
    "        masks = fastremap.refit(masks)\n",
    "        \n",
    "        np.save(mask_path,masks.T)\n",
    "        \n",
    "    items = add_item(image, mask_path, items, \"IHC_TMA\",\"IHC\")\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(items)\n",
    "print(\"{} items added\".format(len(items)))\n",
    "Segmentation_Dataset['Train'] += items[:int(len(items) * 0.8)]\n",
    "Segmentation_Dataset['Validation'] += items[int(len(items) * 0.8):int(len(items) * 0.9)]\n",
    "Segmentation_Dataset['Test'] += items[int(len(items) * 0.9):]\n",
    "\n",
    "show_data(items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MoNuSeg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monuseg_dir= os.path.join(root_dir, \"data\", \"MoNuSeg\")\n",
    "zip_file_path = os.path.join(zip_file_dir , \"MoNuSeg.zip\")\n",
    "\n",
    "if not os.path.exists(monuseg_dir):\n",
    "    extract(zip_file_path, monuseg_dir)\n",
    "    \n",
    "file_path = os.path.join(monuseg_dir,\"monuseg-2018/download\")\n",
    "\n",
    "# Process Test Data\n",
    "items = []\n",
    "set_type = \"test\"\n",
    "test_files = os.listdir(os.path.join(file_path,set_type,\"images\"))\n",
    "\n",
    "for file in tqdm(test_files):\n",
    "    if \".DS_Store\" not in str(file):\n",
    "        image = os.path.join(file_path,set_type,\"images\",file)\n",
    "        masks = str(image).replace(\"images\", \"masks\")\n",
    "        if \".tif\" in image:\n",
    "            im = io.imread(image)\n",
    "            os.remove(image)\n",
    "            image = image.replace(\".tif\",\".png\")\n",
    "            io.imsave(image,im[:,:,:3])\n",
    "            mk = io.imread(masks)\n",
    "            os.remove(masks)\n",
    "            masks = masks.replace(\".tif\",\".npz\")\n",
    "            np.savez_compressed(masks,mk.T)\n",
    "        else:\n",
    "            masks = masks.replace(\".png\",\".npz\")\n",
    "        \n",
    "        items = add_item(image, masks, items, \"MoNuSeg\")\n",
    "        \n",
    "Segmentation_Dataset['Test'] += items[:int(len(items) * 0.5)]\n",
    "Segmentation_Dataset['Validation'] += items[:int(len(items) * 0.5)]\n",
    "\n",
    "# Process Train Data\n",
    "items = []\n",
    "set_type = \"train\"\n",
    "train_files = os.listdir(os.path.join(file_path,set_type,\"images\"))\n",
    "\n",
    "for file in tqdm(train_files):\n",
    "    if \".DS_Store\" not in str(file):\n",
    "        image = os.path.join(file_path,set_type,\"images\",file)\n",
    "        masks = str(image).replace(\"images\", \"masks\")\n",
    "        if \".tif\" in image:\n",
    "            im = io.imread(image)\n",
    "            os.remove(image)\n",
    "            image = image.replace(\".tif\",\".png\")\n",
    "            io.imsave(image,im[:,:,:3])\n",
    "            mk = io.imread(masks)\n",
    "            os.remove(masks)\n",
    "            masks = masks.replace(\".tif\",\".npz\")\n",
    "            np.savez_compressed(masks,mk.T)\n",
    "        else:\n",
    "            masks = masks.replace(\".png\",\".npz\")\n",
    "        \n",
    "        items = add_item(image, masks, items, \"MoNuSeg\")\n",
    "\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(items)\n",
    "print(\"{} items added\".format(len(items)))\n",
    "Segmentation_Dataset['Train'] += items[:int(len(items) * 0.8)]\n",
    "Segmentation_Dataset['Validation'] += items[int(len(items) * 0.8):int(len(items) * 0.9)]\n",
    "\n",
    "show_data(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PANNUKE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pannuke_dir = os.path.join(root_dir, \"data\", \"PanNuke\")\n",
    "if not os.path.exists(pannuke_dir):\n",
    "    os.mkdir(pannuke_dir)\n",
    "\n",
    "for fold in [\"1\", \"2\"]:\n",
    "    if not os.path.exists(os.path.join(pannuke_dir, \"Fold {}\".format(fold))):\n",
    "        if not os.path.exists(os.path.join(pannuke_dir, \"PanNuke_fold_{}.zip\".format(fold))):\n",
    "            zip_file_path = os.path.join(zip_file_dir, \"PanNuke_fold_{}.zip\".format(fold))\n",
    "    \n",
    "            # Download and unzip the dataset\n",
    "            extract(zip_file_path, pannuke_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_data(dataset):\n",
    "\n",
    "    if dataset == \"train\":\n",
    "        fold = \"1\"\n",
    "    elif dataset == \"val\":\n",
    "        fold = \"2\"\n",
    "\n",
    "    create_files=False\n",
    "    out_path = os.path.join(pannuke_dir,\"all_images\")\n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "        create_files=True\n",
    "    out_path = os.path.join(out_path,dataset)\n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "        create_files=True\n",
    "\n",
    "    items = []\n",
    "    masks = np.load(os.path.join(pannuke_dir,\"Fold {}/masks/fold{}/masks.npy\".format(fold,fold)), allow_pickle = True).astype(np.int16)\n",
    "    images = np.load(os.path.join(pannuke_dir,\"Fold {}/images/fold{}/images.npy\".format(fold,fold)), allow_pickle = True).astype(np.uint8)\n",
    "\n",
    "    for i in tqdm(range(len(images))):\n",
    "\n",
    "        assert (np.unique(masks[i,:,:,:-1]) == np.unique(masks[i,:,:,:-1].max(-1))).all()\n",
    "        label = masks[i,:,:,:-1].max(-1)\n",
    "        label,_ = fastremap.renumber(label, in_place=True)\n",
    "        classes = (np.argmax(masks[i,:,:,:-1],axis = -1) + 1) * (label > 0).astype(np.int8)\n",
    "\n",
    "        image = images[i]\n",
    "        relative_path_img = os.path.join(out_path , \"image_\"+str(i)+\".png\")\n",
    "        relative_path_nucleus =os.path.join(out_path , \"nucleus_masks_\"+str(i)+\".npz\")\n",
    "        if create_files:\n",
    "            io.imsave(relative_path_img,image)\n",
    "            np.savez_compressed(relative_path_nucleus, label.T)\n",
    "\n",
    "        items = add_item(relative_path_img, relative_path_nucleus, items, \"PanNuke\")\n",
    "        if fold==1:\n",
    "            print(\"{} items added\".format(len(items)))\n",
    "\n",
    "    return items\n",
    "    \n",
    "items = get_data(\"train\")\n",
    "Segmentation_Dataset['Train']+=items\n",
    "items = get_data(\"val\")\n",
    "Segmentation_Dataset['Validation']+=items[:int(len(items) * 0.5)]\n",
    "Segmentation_Dataset['Test']+=items[int(len(items) * 0.5):]\n",
    "\n",
    "show_data(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action=\"default\")\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "class PrintInputsTransform(MapTransform):\n",
    "    def __init__(self, keys):\n",
    "        super().__init__(keys)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        for key in self.keys:\n",
    "            if key in data:\n",
    "                print(f\"Input for key '{key}':\")\n",
    "                print(data[key].shape)\n",
    "            else:\n",
    "                print(f\"Key '{key}' not found in input data\")\n",
    "        return data\n",
    "\n",
    "roi_size = [256, 256]\n",
    "train_transforms = mt.Compose(\n",
    "    [\n",
    "        mt.LoadImaged(keys=[\"image\", \"label\"],ensure_channel_first=True,image_only=False),\n",
    "        mt.Spacingd(keys=[\"image\", \"label\"], pixdim=(1.0, 1.0, 0.0)),\n",
    "        mt.EnsureTyped(keys=[\"image\", \"label\"], data_type=\"tensor\", dtype=torch.float),\n",
    "        mt.ScaleIntensityd(keys=\"image\", minv=0, maxv=1, channel_wise=True),\n",
    "        mt.ScaleIntensityRangePercentilesd(\n",
    "            keys=\"image\",\n",
    "            lower=1,\n",
    "            upper=99,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            channel_wise=True,\n",
    "            clip=True,\n",
    "        ),\n",
    "        mt.SpatialPadd(keys=[\"image\", \"label\"], spatial_size=roi_size),\n",
    "        mt.RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=roi_size),\n",
    "        mt.RandAffined(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            prob=0.5,\n",
    "            rotate_range=np.pi,\n",
    "            scale_range=[-0.5, 0.5],\n",
    "            mode=[\"bilinear\", \"nearest\"],\n",
    "            spatial_size=roi_size,\n",
    "            cache_grid=True,\n",
    "            padding_mode=\"border\",\n",
    "        ),\n",
    "        mt.RandAxisFlipd(keys=[\"image\", \"label\"], prob=0.5),\n",
    "        mt.RandGaussianNoised(keys=[\"image\"], prob=0.25, mean=0, std=0.1),\n",
    "        mt.RandAdjustContrastd(keys=[\"image\"], prob=0.25, gamma=(1, 2)),\n",
    "        mt.RandGaussianSmoothd(keys=[\"image\"], prob=0.25, sigma_x=(1, 2)),\n",
    "        mt.RandHistogramShiftd(keys=[\"image\"], prob=0.25, num_control_points=3),\n",
    "        mt.RandGaussianSharpend(keys=[\"image\"], prob=0.25),\n",
    "        LabelsToFlows(keys=\"label\", flow_key=\"flow\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = mt.Compose(\n",
    "    [\n",
    "        mt.LoadImaged(keys=[\"image\", \"label\"],ensure_channel_first=True,image_only=False),\n",
    "        mt.Spacingd(keys=[\"image\", \"label\"], pixdim=(1.0, 1.0, 0.0)),\n",
    "        mt.EnsureTyped(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            data_type=\"tensor\",\n",
    "            dtype=torch.float,\n",
    "            allow_missing_keys=True,\n",
    "        ),\n",
    "        mt.ScaleIntensityRangePercentilesd(\n",
    "            keys=\"image\",\n",
    "            lower=1,\n",
    "            upper=99,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            channel_wise=True,\n",
    "            clip=True,\n",
    "        ),\n",
    "        mt.RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=roi_size),\n",
    "        LabelsToFlows(keys=\"label\", flow_key=\"flow\", allow_missing_keys=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Datasets & Dataloaders for training, validation and testing\n",
    "train_dataset = Dataset(data=Segmentation_Dataset['Train'], transform=train_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, drop_last=True, shuffle=True, num_workers=1)\n",
    "\n",
    "val_dataset = Dataset(data=Segmentation_Dataset['Validation'], transform=val_transforms)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=1)\n",
    "\n",
    "# Training loop with validation\n",
    "loss_function = CellLoss()\n",
    "acc_function = CellAcc()\n",
    "\n",
    "# Define the Sliding Window Inferer\n",
    "sliding_inferrer = SlidingWindowInfererAdapt(\n",
    "    roi_size=[256, 256], sw_batch_size=1, overlap=0.25, cache_roi_weight_map=True, progress=False\n",
    ")\n",
    "\n",
    "channels_last = True\n",
    "   \n",
    "ckpt_path = os.path.join(vista2d_root, \"model_checkpoints\")\n",
    "if os.path.exists(ckpt_path) is False:\n",
    "    os.mkdir(ckpt_path)\n",
    "    \n",
    "max_epochs = 5\n",
    "num_epochs_per_validation = 1\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(params=model.parameters(), momentum=0.9, lr=0.01, weight_decay=1e-5)\n",
    "\n",
    "# Amp\n",
    "scaler = GradScaler()\n",
    "amp_dtype = torch.float16\n",
    "\n",
    "best_ckpt_path = os.path.join(ckpt_path, \"model.pt\")\n",
    "intermediate_ckpt_path = os.path.join(ckpt_path, \"model_final.pt\")\n",
    "\n",
    "best_metric = -1\n",
    "start_epoch = 0\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "val_epoch_loss_values = []\n",
    "val_epoch_acc_values = []\n",
    "for epoch in range(0, max_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    model.train()\n",
    "    memory_format = torch.channels_last if channels_last else torch.preserve_format\n",
    "    run_loss = CumulativeAverage()\n",
    "    avg_loss = avg_acc = 0\n",
    "\n",
    "    for idx, batch_data in enumerate(train_loader):\n",
    "        data = batch_data[\"image\"].as_subclass(torch.Tensor).to(memory_format=memory_format, device=device)\n",
    "        target = batch_data[\"flow\"].as_subclass(torch.Tensor).to(memory_format=memory_format, device=device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Use autocast with float16 for mixed precision training\n",
    "        with autocast(dtype=amp_dtype):\n",
    "            logits = model(data)\n",
    "            loss = loss_function(logits.float(), target)\n",
    "\n",
    "        # Use the scaler for backpropagation and optimization\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        batch_size = data.shape[0]\n",
    "        run_loss.append(loss, count=batch_size)\n",
    "        avg_loss = run_loss.aggregate()\n",
    "\n",
    "        if idx % 10==0:\n",
    "            print(f\"Epoch {epoch}/{max_epochs} {idx}/{len(train_loader)} \")\n",
    "            print(f\"loss: {avg_loss:.4f} time {time.time() - start_time:.2f}s \")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss_values.append(avg_loss)\n",
    "\n",
    "    # Validation loop & model checkpoints\n",
    "    if epoch % num_epochs_per_validation == 0:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_run_loss = CumulativeAverage()\n",
    "            run_acc = CumulativeAverage()\n",
    "            for val_idx, val_batch in enumerate(val_loader):\n",
    "                v_data = val_batch[\"image\"].as_subclass(torch.Tensor).to(memory_format=memory_format, device=device)\n",
    "\n",
    "                target = val_batch[\"flow\"].as_subclass(torch.Tensor).to(device=device)\n",
    "\n",
    "                filename = val_batch[\"image\"].meta[ImageMetaKey.FILENAME_OR_OBJ]\n",
    "                batch_size = v_data.shape[0]\n",
    "                loss = acc = None\n",
    "                # Use autocast with float16 for mixed precision validation\n",
    "                with autocast(dtype=amp_dtype):\n",
    "                    logits = sliding_inferrer(inputs=v_data, network=model)\n",
    "                    #    torch.save(logits,os.path.join(root_dir, \"logits.pt\"))\n",
    "                    #    torch.save(target,os.path.join(root_dir, \"target.pt\"))\n",
    "                    val_loss = loss_function(logits, target)\n",
    "\n",
    "                val_run_loss.append(val_loss.to(device=device), count=batch_size)\n",
    "                target = None\n",
    "\n",
    "                pred_mask_all = []\n",
    "\n",
    "                for b_ind in range(logits.shape[0]):  # go over batch dim\n",
    "                    pred_mask = LogitsToLabels()(logits=logits[b_ind], filename=filename)\n",
    "                    pred_mask_all.append(pred_mask)\n",
    "\n",
    "                if acc_function is not None:\n",
    "                    label = val_batch[\"label\"].as_subclass(torch.Tensor)\n",
    "\n",
    "                    for b_ind in range(label.shape[0]):\n",
    "                        acc = acc_function(pred_mask_all[b_ind], label[b_ind, 0].long())\n",
    "                        acc = acc.detach().clone() if isinstance(acc, torch.Tensor) else torch.tensor(acc)\n",
    "\n",
    "                        run_acc.append(acc.to(device=device), count=1)\n",
    "                    label = None\n",
    "\n",
    "                avg_loss = val_loss.cpu() if val_loss is not None else 0\n",
    "                avg_acc = acc.cpu().numpy() if acc is not None else 0\n",
    "\n",
    "            print(f\"Val loss: {val_run_loss.aggregate():.4f} acc {run_acc.aggregate()}  time {time.time() - start_time:.2f}s\")\n",
    "\n",
    "        val_epoch_loss_values.append(val_run_loss.aggregate())\n",
    "        val_epoch_acc_values.append(run_acc.aggregate())\n",
    "        \n",
    "    # Model Saving & Checkpointing\n",
    "    if avg_loss < best_metric:\n",
    "        best_metric = avg_loss\n",
    "        state_dict = model.state_dict()\n",
    "        torch.save({\"state_dict\": state_dict}, best_ckpt_path)\n",
    "        print(f\"Model saved to {best_ckpt_path}\")\n",
    "        \n",
    "print(\"Training Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join(root_dir, \"logits.pt\")):\n",
    "    logits = torch.load(os.path.join(root_dir, \"logits.pt\"))\n",
    "    target = torch.load(os.path.join(root_dir, \"target.pt\"))\n",
    "\n",
    "    print(target.shape,logits.shape)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Handle potential multi-channel \n",
    "    image = np.squeeze(logits[0].cpu().numpy())\n",
    "    label = np.squeeze(target[0].cpu().numpy())\n",
    "    \n",
    "    if image.ndim == 3 and image.shape[0] in [1, 3, 4]:\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "        \n",
    "    axes[0].imshow(image, cmap=\"gray\" if image.ndim == 2 else None)\n",
    "    axes[0].set_title(\"Sample Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "    \n",
    "    label = np.squeeze(label)\n",
    "    if label.ndim == 3 and label.shape[0] in [1, 3, 4]:\n",
    "        label = np.transpose(label, (1, 2, 0))\n",
    "    \n",
    "    if label.ndim == 3 and label.shape[2]>3:\n",
    "        label = label[:,:,:3]\n",
    "        \n",
    "    axes[1].imshow(label)#, cmap=\"viridis\")\n",
    "    axes[1].set_title(\"Ground Truth\")\n",
    "    axes[1].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No outputs to show\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
